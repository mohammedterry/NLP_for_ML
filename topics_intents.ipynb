{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topics_intents.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammedterry/NLP_for_ML/blob/master/topics_intents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "5SLs12EQ6lW3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### preprocessing raw text"
      ]
    },
    {
      "metadata": {
        "id": "zJX_5IZ9vNvb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "example_text = '''\n",
        "A huge fireball exploded in the Earth's atmosphere in December, according to Nasa.\n",
        "\n",
        "The blast was the second largest of its kind in 30 years, and the biggest since the fireball over Chelyabinsk in Russia six years ago.\n",
        "\n",
        "But it went largely unnoticed until now because it blew up over the Bering Sea, off Russia's Kamchatka Peninsula.\n",
        "\n",
        "The space rock exploded with 10 times the energy released by the Hiroshima atomic bomb.\n",
        "\n",
        "Lindley Johnson, planetary defence officer at Nasa, told BBC News a fireball this big is only expected about two or three times every 100 years.\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SOFDnlV1sYoy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e8e76b2d-8b44-41d3-99e0-b935cb660a4c"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stopWords = stopwords.words('english')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wn_lemmatiser = WordNetLemmatizer()\n",
        "\n",
        "def clean(text):\n",
        "  return ' '.join(''.join(letter if ord('a') <= ord(letter) <= ord('z') or letter.isdigit() else ' ' for letter in text.lower()).split())\n",
        "\n",
        "def preprocess(document):\n",
        "  return [wn_lemmatiser.lemmatize(word,pos='v') for word in clean(document).split() if word not in stopWords]"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B34O9tNY7S03",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "aabac787-8309-4e27-89c2-e0c0d63cdd19"
      },
      "cell_type": "code",
      "source": [
        "' '.join(preprocess(example_text))"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'huge fireball explode earth atmosphere december accord nasa blast second largest kind 30 years biggest since fireball chelyabinsk russia six years ago go largely unnoticed blow bering sea russia kamchatka peninsula space rock explode 10 time energy release hiroshima atomic bomb lindley johnson planetary defence officer nasa tell bbc news fireball big expect two three time every 100 years'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "metadata": {
        "id": "aan_QRxzHTWz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LDA (Gensim)\n"
      ]
    },
    {
      "metadata": {
        "id": "9TwLltwr7s2e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pre-trained LDA models"
      ]
    },
    {
      "metadata": {
        "id": "u-6vkALx6OLL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "load some pretrained lda models"
      ]
    },
    {
      "metadata": {
        "id": "cmBci1Xf6VyD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import datapath\n",
        "import gensim\n",
        "\n",
        "lda1 = gensim.models.LdaMulticore.load(datapath(\"lda_3_0_1_model\"))\n",
        "lda2 = gensim.models.LdaMulticore.load(datapath(\"ldamodel_python_3_5\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7JGzT-Mq6eO9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lda_topics(text, model, top_n = 3):\n",
        "  results = dict(model[model.id2word.doc2bow(preprocess(text))])\n",
        "  return[(topic,'/'.join([w for w in model.print_topic(topic,5).split('\"') if w.isalpha()]), score) for score,topic in sorted(zip(results.values(), results.keys()), reverse=True)][:top_n]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4cHR4bTT7vdE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "try them out"
      ]
    },
    {
      "metadata": {
        "id": "k2lZuQpE61x4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fa69e50b-1240-4a0a-cfca-ce2ee235bf69"
      },
      "cell_type": "code",
      "source": [
        "lda_topics(example_text, lda1)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 'system/user/eps/time/response', 0.8268186987472599),\n",
              " (1, 'graph/trees/minors/survey/interface', 0.17318130125274017)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "metadata": {
        "id": "gK2mU_du60Ol",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bf120cf7-1869-490b-99ad-a2695a5ec0e3"
      },
      "cell_type": "code",
      "source": [
        "lda_topics(example_text, lda2)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 'response/user/eps/survey/time', 0.8268134549578768),\n",
              " (0, 'graph/trees/minors/system/computer', 0.17318654504212305)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "metadata": {
        "id": "J1uyBY7p67aV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train your own LDA model"
      ]
    },
    {
      "metadata": {
        "id": "bUgQCfbBy5tu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Get some sample documents to represent the topics"
      ]
    },
    {
      "metadata": {
        "id": "_S_lEWYjHWCP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', shuffle = True, remove=('headers', 'footers', 'quotes'))\n",
        "preprocessed_docs = [preprocess(doc) for doc in newsgroups_train.data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tS3UOCO1y9hu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "train an LDA model on the sample documents"
      ]
    },
    {
      "metadata": {
        "id": "jEvNolZtuIuF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "vocab = gensim.corpora.Dictionary(preprocessed_docs)\n",
        "vocab.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)\n",
        "bow_corpus = [vocab.doc2bow(doc) for doc in preprocessed_docs]\n",
        "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = 20, id2word = vocab, passes = 10, workers = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2q0_0_nczCJG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "save the trained LDA model"
      ]
    },
    {
      "metadata": {
        "id": "M50LUq08xyQ8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import datapath\n",
        "temp_file = datapath(\"lda\")\n",
        "lda_model.save(temp_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VG86FhoGzEit",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "load in the trained LDA model"
      ]
    },
    {
      "metadata": {
        "id": "4QaiAJZnyMQL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "your_lda = gensim.models.LdaMulticore.load(temp_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7dGFGQBOzG5F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "test out the trained LDA model"
      ]
    },
    {
      "metadata": {
        "id": "C_L_LGU97ICu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a6f60f47-b29b-40d0-df0b-3a7b67f171c2"
      },
      "cell_type": "code",
      "source": [
        "lda_topics(example_text, your_lda)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3, 'gun/government/us/really/keep', 0.33644345),\n",
              " (17, 'state/program/report/national/research', 0.27499065),\n",
              " (2, 'wire/cause/power/back/grind', 0.16544925)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "metadata": {
        "id": "2MC1v89bndZm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Intent Extraction (NLP Architect)"
      ]
    },
    {
      "metadata": {
        "id": "TzhSiBmIUSiX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train your own Intent Model"
      ]
    },
    {
      "metadata": {
        "id": "xQVx_k1CoUyb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "install the nlp architect library"
      ]
    },
    {
      "metadata": {
        "id": "1K5mzJH9oXUg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install nlp_architect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QMAM4Lq2nw0g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "download and load in the intent training data"
      ]
    },
    {
      "metadata": {
        "id": "z3S8QhXin1OO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "33901953-c44b-474b-b76d-de0bc09a06ee"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/snipsco/nlu-benchmark.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nlu-benchmark'...\n",
            "remote: Enumerating objects: 378, done.\u001b[K\n",
            "remote: Total 378 (delta 0), reused 0 (delta 0), pack-reused 378\u001b[K\n",
            "Receiving objects: 100% (378/378), 1.23 MiB | 152.00 KiB/s, done.\n",
            "Resolving deltas: 100% (240/240), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c94hiXTEoMjq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nlp_architect.data.intent_datasets import SNIPS\n",
        "intent_dataset = SNIPS(path='nlu-benchmark/2017-06-custom-intent-engines/',sentence_length=50,word_length=12)\n",
        "train_x, train_c, train_i, train_y = intent_dataset.train_set\n",
        "test_x, test_c, test_i, test_y = intent_dataset.test_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1uZ3Sp6OvGUP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "encode training labels as one-hot vectors"
      ]
    },
    {
      "metadata": {
        "id": "FhlgPZeUvBTX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.utils import to_categorical\n",
        "train_y = to_categorical(train_y, intent_dataset.label_vocab_size)\n",
        "test_y = to_categorical(test_y, intent_dataset.label_vocab_size)\n",
        "\n",
        "from nlp_architect.utils.generic import one_hot\n",
        "train_i = one_hot(train_i, len(intent_dataset.intents_vocab))\n",
        "test_i = one_hot(test_i, len(intent_dataset.intents_vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GrGCv5Lgsx4Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "download and load in word vectors (e.g. GloVe - feel free to use others)"
      ]
    },
    {
      "metadata": {
        "id": "zp0ukxseuDv0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "3f18cdb1-cd5f-4a68-f059-1a63f440031d"
      },
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-19 11:31:44--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-03-19 11:31:44--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  14.6MB/s    in 57s     \n",
            "\n",
            "2019-03-19 11:32:41 (14.5 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vgA6VsUxq5CO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nlp_architect.utils.embedding import load_word_embeddings\n",
        "wordvectors, _ = load_word_embeddings('glove.6B.300d.txt')\n",
        "\n",
        "from nlp_architect.utils.embedding import get_embedding_matrix\n",
        "embedding_matrix = get_embedding_matrix(wordvectors, intent_dataset.word_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j7kBmsE_u1J3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "set up and train the model (RNN - feel free to modify the hyperparameters, and be sure to match word_emb_dims to the size of word vectors used (e.g. if using Glove.100, then word_emb_dims= 100) )"
      ]
    },
    {
      "metadata": {
        "id": "FUREQjFUsSkt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nlp_architect.models.intent_extraction import MultiTaskIntentModel\n",
        "machinelearning_model = MultiTaskIntentModel()\n",
        "machinelearning_model.build(intent_dataset.word_len, intent_dataset.label_vocab_size, intent_dataset.intent_size,intent_dataset.word_vocab_size,intent_dataset.char_vocab_size,word_emb_dims=300,tagger_lstm_dims=100, dropout=0.2)\n",
        "machinelearning_model.load_embedding_weights(embedding_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cq0HVWtzwbIk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_inputs = [train_x, train_c]\n",
        "test_inputs = [test_x, test_c]\n",
        "\n",
        "train_outputs = [train_i, train_y]\n",
        "test_outputs = [test_i, test_y]\n",
        "\n",
        "machinelearning_model.fit(train_inputs, train_outputs, batch_size = 32, epochs = 50, validation = (test_inputs, test_outputs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9DDCAEBSx6Cm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "lets try out our trained model"
      ]
    },
    {
      "metadata": {
        "id": "HfLh89N8x8Rz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "sp = spacy.load('en')\n",
        "from spacy.tokenizer import Tokenizer\n",
        "tokenizer = Tokenizer(sp.vocab)\n",
        "\n",
        "def predict_intent(sentence, max_words = 50, max_chars = 12):\n",
        "  tokens = [token.text for token in tokenizer(sentence)[:max_words]]\n",
        "  x = [intent_dataset.word_vocab[token] if token in intent_dataset.word_vocab else 1 for token in tokens]\n",
        "  x += [0] * (max_words - len(x))\n",
        "  \n",
        "  c = [[intent_dataset.char_vocab[ch] for ch in word[:max_chars]] + [0]*(max_chars-len(word[:max_chars])) for word in sentence.split()[:max_words]]\n",
        "  c += [[0]*max_chars for _ in range(max_words - len(c))]\n",
        "  \n",
        "  inputs = [[x], [c]]\n",
        "  predictions = machinelearning_model.predict(inputs, batch_size=1)\n",
        "  predicted_intents = predictions[0].argmax(1)\n",
        "  return [intent_dataset.intents_vocab.id_to_word(i) for i in predicted_intents]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TZG3JD3YC-Qz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ae982ef-1589-4fb0-db85-b31b10d0c742"
      },
      "cell_type": "code",
      "source": [
        "predict_intent(\"play a little song\")"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PlayMusic']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "metadata": {
        "id": "MnsWgOWttJjz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ShortText"
      ]
    },
    {
      "metadata": {
        "id": "CZMIzWya9JP_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip3 install shorttext\n",
        "#!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
        "#from shorttext.utils import load_word2vec_model\n",
        "#wvmodel = load_word2vec_model('GoogleNews-vectors-negative300.bin.gz')\n",
        "\n",
        "#import shorttext\n",
        "#nihtraindata = shorttext.data.nihreports(sample_size=None)\n",
        "#classifier = shorttext.classifiers.SumEmbeddedVecClassifier(wvmodel)   \n",
        "#classifier.train(nihtraindata)\n",
        "#classifier.save_compact_model('sumvec_nihdata_model.bin')\n",
        "#!ls\n",
        "\n",
        "#classifier2 = shorttext.classifiers.load_sumword2vec_classifier(wvmodel, 'sumvec_nihdata_model.bin')\n",
        "\n",
        "#classifier2.score('bioinformatics')\n",
        "\n",
        "#sorted(classifier2.score('cancer immunotherapy').items(), key=lambda item: item[1], reverse=True)[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}