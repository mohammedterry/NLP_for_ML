{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topics_intents.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammedterry/NLP_for_ML/blob/master/topics_intents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "5SLs12EQ6lW3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### preprocessing raw text"
      ]
    },
    {
      "metadata": {
        "id": "zJX_5IZ9vNvb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "example_text = '''\n",
        "A huge fireball exploded in the Earth's atmosphere in December, according to Nasa.\n",
        "\n",
        "The blast was the second largest of its kind in 30 years, and the biggest since the fireball over Chelyabinsk in Russia six years ago.\n",
        "\n",
        "But it went largely unnoticed until now because it blew up over the Bering Sea, off Russia's Kamchatka Peninsula.\n",
        "\n",
        "The space rock exploded with 10 times the energy released by the Hiroshima atomic bomb.\n",
        "\n",
        "Lindley Johnson, planetary defence officer at Nasa, told BBC News a fireball this big is only expected about two or three times every 100 years.\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SOFDnlV1sYoy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stopWords = stopwords.words('english')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wn_lemmatiser = WordNetLemmatizer()\n",
        "\n",
        "def clean(text):\n",
        "  return ' '.join(''.join(letter if ord('a') <= ord(letter) <= ord('z') or letter.isdigit() else ' ' for letter in text.lower()).split())\n",
        "\n",
        "def preprocess(document):\n",
        "  return [[wn_lemmatiser.lemmatize(word,pos='v') for word in clean(sentence).split() if word not in stopWords] for sentence in nltk.sent_tokenize(document)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B34O9tNY7S03",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preprocess(example_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8tt3-A-BbClA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# WordNet (NLTK)"
      ]
    },
    {
      "metadata": {
        "id": "PsataIVkbFdu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def get_everything_related_to(word):\n",
        "    word = '_'.join(word.split())\n",
        "    return (\n",
        "          {name.replace('_',' ').lower() for synset in wn.synsets(word) for name in synset.lemma_names()}\n",
        "        | {name.replace('_',' ').lower() for synset in wn.synsets(word) for hypernym in synset.hypernyms() for name in hypernym.lemma_names()}\n",
        "        | {name.replace('_',' ').lower() for synset in wn.synsets(word) for hyponym in synset.hyponyms() for name in hyponym.lemma_names()}\n",
        "        | {name.replace('_',' ').lower() for synset in wn.synsets(word) for part_meronym in synset.part_meronyms() for name in part_meronym.lemma_names()}\n",
        "        | {name.replace('_',' ').lower() for synset in wn.synsets(word) for substance_meronym in synset.substance_meronyms() for name in substance_meronym.lemma_names()}\n",
        "        | {name.replace('_',' ').lower() for synset in wn.synsets(word) for member_meronym in synset.member_meronyms() for name in member_meronym.lemma_names()}\n",
        "        | {name.replace('_',' ').lower() for synset in wn.synsets(word) for part_holonym in synset.part_holonyms() for name in part_holonym.lemma_names()}\n",
        "        | {name.replace('_',' ').lower() for synset in wn.synsets(word) for substance_holonym in synset.substance_holonyms() for name in substance_holonym.lemma_names()}\n",
        "        | {name.replace('_',' ').lower() for synset in wn.synsets(word) for member_holonym in synset.member_holonyms() for name in member_holonym.lemma_names()}\n",
        "        | {name.replace('_',' ').lower() for synset in wn.synsets(word) for topic_domain in synset.topic_domains() for name in topic_domain.lemma_names()}\n",
        "        | {name.replace('_',' ').lower() for synset in wn.synsets(word) for region_domain in synset.region_domains() for name in region_domain.lemma_names()}\n",
        "        | {name.replace('_',' ').lower() for synset in wn.synsets(word) for usage_domain in synset.usage_domains() for name in usage_domain.lemma_names()}\n",
        "        | {name.replace('_',' ').lower() for synset in wn.synsets(word) for entailment in synset.entailments() for name in entailment.lemma_names()}\n",
        "        | {name.replace('_',' ').lower() for synset in wn.synsets(word) for cause in synset.causes() for name in cause.lemma_names()}\n",
        "        | {name.replace('_',' ').lower() for synset in wn.synsets(word) for also_see in synset.also_sees() for name in also_see.lemma_names()}\n",
        "        | {name.replace('_',' ').lower() for synset in wn.synsets(word) for verb_group in synset.verb_groups() for name in verb_group.lemma_names()}\n",
        "        | {name.replace('_',' ').lower() for synset in wn.synsets(word) for similar in synset.similar_tos() for name in similar.lemma_names()}\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e3YI2z5ccHp4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_everything_related_to(\"artificial intelligence\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8fqSNUtmcoB4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Topic Spotting"
      ]
    },
    {
      "metadata": {
        "id": "Ygng6mJVcerc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mini_knowledge_graph = {word:topic for topic in (\"space\", \"travel\",\"meteor\") for word in get_everything_related_to(topic)}\n",
        "\n",
        "for i,sentence in enumerate(preprocess(example_text)):\n",
        "  print(f\"\\nSentence {i}: {' '.join(sentence)}\")\n",
        "  for word in sentence:\n",
        "    if word in mini_knowledge_graph:\n",
        "      print(f\"\\t({mini_knowledge_graph[word].upper()}) {word}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6k_UYhfwenhM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Common Topics (without specifying which)"
      ]
    },
    {
      "metadata": {
        "id": "2QLpv16_eqC5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def find_topics(text,top_n = 10):\n",
        "  counters = []\n",
        "  for sentence in preprocess(text):\n",
        "    counters.append({})\n",
        "    for word in sentence:\n",
        "      for topic in get_everything_related_to(word):\n",
        "        if topic in counters[-1]:\n",
        "          counters[-1][topic] += 1\n",
        "        else:\n",
        "          counters[-1][topic] = 1\n",
        "        for rel_topic in get_everything_related_to(topic):\n",
        "          if rel_topic in counters[-1]:\n",
        "            counters[-1][rel_topic] += 1\n",
        "          else:\n",
        "            counters[-1][rel_topic] = 1    \n",
        "  return ['/'.join([topic for c,topic in sorted(zip(counter.values(), counter.keys()),reverse=True) if c > 1][:top_n]) for counter in counters]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6KiAphgggcDa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "find_topics(example_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nlb1xDQPbF8u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Clustering (HDBScan)"
      ]
    },
    {
      "metadata": {
        "id": "NS6YtZib0MKX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "display word vectors and clusters"
      ]
    },
    {
      "metadata": {
        "id": "mCSTT0fOwev2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "def plot_clusters(words, svectors, clusters):\n",
        "  from sklearn.decomposition import PCA\n",
        "  pca = PCA(n_components=3)\n",
        "  principalComponents = pca.fit_transform(svectors)\n",
        "  \n",
        "  import pandas as pd\n",
        "  dataframe = pd.DataFrame(data = principalComponents, columns = ['x', 'y','z'])\n",
        "\n",
        "  import seaborn as sns\n",
        "  sns.set_style(\"whitegrid\")\n",
        "  dataframe['colour'] = pd.Series([sns.color_palette()[i] for i in clusters])\n",
        "  \n",
        "  import matplotlib.pyplot as plt, mpl_toolkits.mplot3d\n",
        "  fig = plt.figure()\n",
        "  graph = fig.gca(projection='3d')\n",
        "  [graph.text(x, z, y, s, 'x', color = c) for x, y, z, s, c in zip(dataframe['x'], dataframe['y'], dataframe['z'], words, dataframe['colour'])]\n",
        "  graph.set_xlim(min(dataframe['x']), max(dataframe['x']))\n",
        "  graph.set_ylim(min(dataframe['z']), max(dataframe['z']))\n",
        "  graph.set_zlim(min(dataframe['y']), max(dataframe['y']))\n",
        "  plt.show()\n",
        "  \n",
        "  graph2=sns.regplot(data=dataframe, x=\"x\", y=\"y\", fit_reg=False, marker=\"+\")\n",
        "  [graph2.text(x, y, s,color=c) for x,y,s,c in zip(dataframe['x'],dataframe['y'],words,dataframe[\"colour\"])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kwxuvKe-xyb9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "convert words to vectors (e.g. spacy - feel free to use others)"
      ]
    },
    {
      "metadata": {
        "id": "wyaqvAWCxUwK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "sp = spacy.load('en')\n",
        "sp_words = [word for sentence in preprocess(example_text) for word in sentence]\n",
        "sp_vectors = [sp(word).vector for word in sp_words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YhkBrkk8x5Mv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "cluster the word vectors using a density based algorithm (number of clusters are automatically determined by the algorithm)"
      ]
    },
    {
      "metadata": {
        "id": "tH09SxoKx4he",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install hdbscan\n",
        "from hdbscan import HDBSCAN\n",
        "clusterer = HDBSCAN()\n",
        "clusterer.fit(sp_vectors)\n",
        "clusterer.labels_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l-1jSmc1yyx0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_clusters(sp_words, sp_vectors, clusterer.labels_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zLJiZNNoxFbE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def name_clusters(words,cluster_labels):\n",
        "  def rank(words,top_n = 3):\n",
        "    counter = {}\n",
        "    for word in words:\n",
        "      if word in counter:\n",
        "        counter[word] += 1\n",
        "      else:\n",
        "        counter[word] = 1\n",
        "    return '/'.join([word for count,word in sorted(zip(counter.values(), counter.keys()), reverse=True)][:top_n])\n",
        "\n",
        "  clusters = {cluster:[] for cluster in set(clusterer.labels_)}\n",
        "\n",
        "  for cluster,word in zip(cluster_labels,words):\n",
        "    clusters[cluster].append(word)\n",
        "\n",
        "  return [rank(words) for cluster,words in clusters.items() if cluster > -1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tGSyjR81-oXg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "name_clusters(sp_words,clusterer.labels_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aan_QRxzHTWz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LDA (Gensim)\n"
      ]
    },
    {
      "metadata": {
        "id": "9TwLltwr7s2e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pre-trained LDA models"
      ]
    },
    {
      "metadata": {
        "id": "u-6vkALx6OLL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "load some pretrained lda models"
      ]
    },
    {
      "metadata": {
        "id": "cmBci1Xf6VyD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import datapath\n",
        "import gensim\n",
        "\n",
        "lda1 = gensim.models.LdaMulticore.load(datapath(\"lda_3_0_1_model\"))\n",
        "lda2 = gensim.models.LdaMulticore.load(datapath(\"ldamodel_python_3_5\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7JGzT-Mq6eO9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lda_topics(text, model, top_n = 3):\n",
        "  results = [dict(model[model.id2word.doc2bow(sentence)]) for sentence in preprocess(text)]\n",
        "  return [[(topic,'/'.join([w for w in model.print_topic(topic,5).split('\"') if w.isalpha()]), score) for score,topic in sorted(zip(result.values(), result.keys()), reverse=True)][:top_n] for result in results]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4cHR4bTT7vdE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "try them out"
      ]
    },
    {
      "metadata": {
        "id": "k2lZuQpE61x4",
        "colab_type": "code",
        "outputId": "dc61a018-988c-4fcf-93a7-b6e6d9feb6a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "lda_topics(example_text, lda1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(1, 'graph/trees/minors/survey/interface', 0.5),\n",
              "  (0, 'system/user/eps/time/response', 0.5)],\n",
              " [(1, 'graph/trees/minors/survey/interface', 0.5),\n",
              "  (0, 'system/user/eps/time/response', 0.5)],\n",
              " [(1, 'graph/trees/minors/survey/interface', 0.5),\n",
              "  (0, 'system/user/eps/time/response', 0.5)],\n",
              " [(0, 'system/user/eps/time/response', 0.7404856864799025),\n",
              "  (1, 'graph/trees/minors/survey/interface', 0.2595143135200974)],\n",
              " [(0, 'system/user/eps/time/response', 0.7404743324918834),\n",
              "  (1, 'graph/trees/minors/survey/interface', 0.2595256675081165)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "metadata": {
        "id": "gK2mU_du60Ol",
        "colab_type": "code",
        "outputId": "29bb945e-f4c0-4848-859b-537d7181e314",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "lda_topics(example_text, lda2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(1, 'response/user/eps/survey/time', 0.5),\n",
              "  (0, 'graph/trees/minors/system/computer', 0.5)],\n",
              " [(1, 'response/user/eps/survey/time', 0.5),\n",
              "  (0, 'graph/trees/minors/system/computer', 0.5)],\n",
              " [(1, 'response/user/eps/survey/time', 0.5),\n",
              "  (0, 'graph/trees/minors/system/computer', 0.5)],\n",
              " [(1, 'response/user/eps/survey/time', 0.7405229458090367),\n",
              "  (0, 'graph/trees/minors/system/computer', 0.2594770541909634)],\n",
              " [(1, 'response/user/eps/survey/time', 0.7404747732489492),\n",
              "  (0, 'graph/trees/minors/system/computer', 0.2595252267510509)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "metadata": {
        "id": "J1uyBY7p67aV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train your own LDA model"
      ]
    },
    {
      "metadata": {
        "id": "bUgQCfbBy5tu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Get some sample documents to represent the topics"
      ]
    },
    {
      "metadata": {
        "id": "_S_lEWYjHWCP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', shuffle = True, remove=('headers', 'footers', 'quotes'))\n",
        "preprocessed_docs = [[word for sentence in preprocess(doc) for word in sentence] for doc in newsgroups_train.data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tS3UOCO1y9hu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "train an LDA model on the sample documents"
      ]
    },
    {
      "metadata": {
        "id": "jEvNolZtuIuF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "vocab = gensim.corpora.Dictionary(preprocessed_docs)\n",
        "vocab.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)\n",
        "bow_corpus = [vocab.doc2bow(doc) for doc in preprocessed_docs]\n",
        "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = 20, id2word = vocab, passes = 10, workers = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2q0_0_nczCJG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "save the trained LDA model"
      ]
    },
    {
      "metadata": {
        "id": "M50LUq08xyQ8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import datapath\n",
        "temp_file = datapath(\"lda\")\n",
        "lda_model.save(temp_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VG86FhoGzEit",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "load in the trained LDA model"
      ]
    },
    {
      "metadata": {
        "id": "4QaiAJZnyMQL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "your_lda = gensim.models.LdaMulticore.load(temp_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7dGFGQBOzG5F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "test out the trained LDA model"
      ]
    },
    {
      "metadata": {
        "id": "C_L_LGU97ICu",
        "colab_type": "code",
        "outputId": "25b86b45-e29e-44bb-c2d8-0d4e35c969a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "lda_topics(example_text, your_lda)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(9, 'space/armenian/turkish/university/armenians', 0.88124996)],\n",
              " [(3, 'q/mr/president/state/government', 0.30413043),\n",
              "  (16, 'game/team/play/year/season', 0.2645308),\n",
              "  (12, 'tell/us/wire/back/start', 0.26439306)],\n",
              " [(9, 'space/armenian/turkish/university/armenians', 0.81),\n",
              "  (5, 'row/germany/op/int', 0.010000002),\n",
              "  (1, 'israel/greek/law/word/mean', 0.010000002)],\n",
              " [(9, 'space/armenian/turkish/university/armenians', 0.89444447)],\n",
              " [(9, 'space/armenian/turkish/university/armenians', 0.67251325),\n",
              "  (12, 'tell/us/wire/back/start', 0.26320103)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 220
        }
      ]
    },
    {
      "metadata": {
        "id": "2MC1v89bndZm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Intent Extraction (NLP Architect)"
      ]
    },
    {
      "metadata": {
        "id": "TzhSiBmIUSiX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train your own Intent Model"
      ]
    },
    {
      "metadata": {
        "id": "xQVx_k1CoUyb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "install the nlp architect library"
      ]
    },
    {
      "metadata": {
        "id": "1K5mzJH9oXUg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install nlp_architect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QMAM4Lq2nw0g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "download and load in the intent training data"
      ]
    },
    {
      "metadata": {
        "id": "z3S8QhXin1OO",
        "colab_type": "code",
        "outputId": "33901953-c44b-474b-b76d-de0bc09a06ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/snipsco/nlu-benchmark.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nlu-benchmark'...\n",
            "remote: Enumerating objects: 378, done.\u001b[K\n",
            "remote: Total 378 (delta 0), reused 0 (delta 0), pack-reused 378\u001b[K\n",
            "Receiving objects: 100% (378/378), 1.23 MiB | 152.00 KiB/s, done.\n",
            "Resolving deltas: 100% (240/240), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c94hiXTEoMjq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nlp_architect.data.intent_datasets import SNIPS\n",
        "intent_dataset = SNIPS(path='nlu-benchmark/2017-06-custom-intent-engines/',sentence_length=50,word_length=12)\n",
        "train_x, train_c, train_i, train_y = intent_dataset.train_set\n",
        "test_x, test_c, test_i, test_y = intent_dataset.test_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1uZ3Sp6OvGUP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "encode training labels as one-hot vectors"
      ]
    },
    {
      "metadata": {
        "id": "FhlgPZeUvBTX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.utils import to_categorical\n",
        "train_y = to_categorical(train_y, intent_dataset.label_vocab_size)\n",
        "test_y = to_categorical(test_y, intent_dataset.label_vocab_size)\n",
        "\n",
        "from nlp_architect.utils.generic import one_hot\n",
        "train_i = one_hot(train_i, len(intent_dataset.intents_vocab))\n",
        "test_i = one_hot(test_i, len(intent_dataset.intents_vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GrGCv5Lgsx4Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "download and load in word vectors (e.g. GloVe - feel free to use others)"
      ]
    },
    {
      "metadata": {
        "id": "zp0ukxseuDv0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vgA6VsUxq5CO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nlp_architect.utils.embedding import load_word_embeddings\n",
        "wordvectors, _ = load_word_embeddings('glove.6B.300d.txt')\n",
        "\n",
        "from nlp_architect.utils.embedding import get_embedding_matrix\n",
        "embedding_matrix = get_embedding_matrix(wordvectors, intent_dataset.word_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j7kBmsE_u1J3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "set up and train the model (RNN - feel free to modify the hyperparameters, and be sure to match word_emb_dims to the size of word vectors used (e.g. if using Glove.100, then word_emb_dims= 100) )"
      ]
    },
    {
      "metadata": {
        "id": "FUREQjFUsSkt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nlp_architect.models.intent_extraction import MultiTaskIntentModel\n",
        "machinelearning_model = MultiTaskIntentModel()\n",
        "machinelearning_model.build(intent_dataset.word_len, intent_dataset.label_vocab_size, intent_dataset.intent_size,intent_dataset.word_vocab_size,intent_dataset.char_vocab_size,word_emb_dims=300,tagger_lstm_dims=100, dropout=0.2)\n",
        "machinelearning_model.load_embedding_weights(embedding_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cq0HVWtzwbIk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_inputs = [train_x, train_c]\n",
        "test_inputs = [test_x, test_c]\n",
        "\n",
        "train_outputs = [train_i, train_y]\n",
        "test_outputs = [test_i, test_y]\n",
        "\n",
        "machinelearning_model.fit(train_inputs, train_outputs, batch_size = 32, epochs = 50, validation = (test_inputs, test_outputs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9DDCAEBSx6Cm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "lets try out our trained model"
      ]
    },
    {
      "metadata": {
        "id": "HfLh89N8x8Rz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "sp = spacy.load('en')\n",
        "from spacy.tokenizer import Tokenizer\n",
        "tokenizer = Tokenizer(sp.vocab)\n",
        "\n",
        "def predict_intent(sentence, max_words = 50, max_chars = 12):\n",
        "  tokens = [token.text for token in tokenizer(sentence)[:max_words]]\n",
        "  x = [intent_dataset.word_vocab[token] if token in intent_dataset.word_vocab else 1 for token in tokens]\n",
        "  x += [0] * (max_words - len(x))\n",
        "  \n",
        "  c = [[intent_dataset.char_vocab[ch] for ch in word[:max_chars]] + [0]*(max_chars-len(word[:max_chars])) for word in sentence.split()[:max_words]]\n",
        "  c += [[0]*max_chars for _ in range(max_words - len(c))]\n",
        "  \n",
        "  inputs = [[x], [c]]\n",
        "  predictions = machinelearning_model.predict(inputs, batch_size=1)\n",
        "  predicted_intents = predictions[0].argmax(1)\n",
        "  return [intent_dataset.intents_vocab.id_to_word(i) for i in predicted_intents]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TZG3JD3YC-Qz",
        "colab_type": "code",
        "outputId": "8ae982ef-1589-4fb0-db85-b31b10d0c742",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "predict_intent(\"play a little song\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PlayMusic']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "metadata": {
        "id": "MnsWgOWttJjz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ShortText"
      ]
    },
    {
      "metadata": {
        "id": "CZMIzWya9JP_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip3 install shorttext\n",
        "#!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
        "#from shorttext.utils import load_word2vec_model\n",
        "#wvmodel = load_word2vec_model('GoogleNews-vectors-negative300.bin.gz')\n",
        "\n",
        "#import shorttext\n",
        "#nihtraindata = shorttext.data.nihreports(sample_size=None)\n",
        "#classifier = shorttext.classifiers.SumEmbeddedVecClassifier(wvmodel)   \n",
        "#classifier.train(nihtraindata)\n",
        "#classifier.save_compact_model('sumvec_nihdata_model.bin')\n",
        "#!ls\n",
        "\n",
        "#classifier2 = shorttext.classifiers.load_sumword2vec_classifier(wvmodel, 'sumvec_nihdata_model.bin')\n",
        "\n",
        "#classifier2.score('bioinformatics')\n",
        "\n",
        "#sorted(classifier2.score('cancer immunotherapy').items(), key=lambda item: item[1], reverse=True)[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}