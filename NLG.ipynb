{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLG.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammedterry/NLP_for_ML/blob/master/NLG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MV3k0ORgkOz",
        "colab_type": "text"
      },
      "source": [
        "# GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8INhHKGgnJb",
        "colab_type": "code",
        "outputId": "790069e3-0d7d-4e8f-f253-f9d7a7823f17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "!git clone https://github.com/openai/gpt-2\n",
        "import os\n",
        "os.chdir('gpt-2')\n",
        "!pip3 install -r requirements.txt\n",
        "!python3 download_model.py 345M"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 174, done.\u001b[K\n",
            "remote: Total 174 (delta 0), reused 0 (delta 0), pack-reused 174\u001b[K\n",
            "Receiving objects: 100% (174/174), 4.35 MiB | 8.41 MiB/s, done.\n",
            "Resolving deltas: 100% (90/90), done.\n",
            "Requirement already satisfied: fire>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.1.3)\n",
            "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (2017.4.5)\n",
            "Requirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Requirement already satisfied: tqdm==4.31.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.31.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Fetching checkpoint: 1.00kit [00:00, 654kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 48.4Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 647kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:29, 48.1Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 5.64Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 41.9Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 39.1Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX21RRRtVaXv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "outputId": "7a37d2ef-ca51-482c-d9a2-88170b9f9a4e"
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --model_name=345M --nsamples=3 --length=30 --top_k 40 --temperature 0.7"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-14 00:23:41.181344: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-14 00:23:41.181822: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2f5d080 executing computations on platform Host. Devices:\n",
            "2019-05-14 00:23:41.181877: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/gpt-2/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/gpt-2/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2019-05-14 00:23:51.043804: W tensorflow/core/framework/allocator.cc:124] Allocation of 205852672 exceeds 10% of system memory.\n",
            "Model prompt >>> curiouser and curiouser\n",
            "======================================== SAMPLE 1 ========================================\n",
            ", the more I'm sure it's a good thing. That's because the problem with the story is that it's not really about how bad it\n",
            "======================================== SAMPLE 2 ========================================\n",
            ", in the eyes of the world.\n",
            "\n",
            "And that's why the world, of course, is hungry for even more of this nonsense. And\n",
            "======================================== SAMPLE 3 ========================================\n",
            ", who could not, it was said, be ignorant of the truth, but had seen and heard it, and could make no other conclusion; and\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 68, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 86, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 127, in Fire\n",
            "    component_trace = _Fire(component, args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 366, in _Fire\n",
            "    component, remaining_args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 542, in _CallCallable\n",
            "    result = fn(*varargs, **kwargs)\n",
            "  File \"src/interactive_conditional_samples.py\", line 83, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1569, in __exit__\n",
            "    if exec_type is errors.OpError:\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz2kaKKK92Ls",
        "colab_type": "text"
      },
      "source": [
        "# Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XFYSeHW3cjm",
        "colab_type": "code",
        "outputId": "11e9261c-141b-4331-8b60-b89c38f37bc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "sentences = gutenberg.sents('carroll-alice.txt') #'shakespeare-macbeth.txt') #"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1kpCGiK366Q",
        "colab_type": "code",
        "outputId": "a3fda132-01c0-4c0d-9650-1da2da1bdee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1765
        }
      },
      "source": [
        "text = '\\n'.join([' '.join(sentence) for sentence in sentences][100:200])\n",
        "print(text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I ' m sure _I_ shan ' t be able !\n",
            "I shall be a great deal too far off to trouble myself about you : you must manage the best way you can ;-- but I must be kind to them ,' thought Alice , ' or perhaps they won ' t walk the way I want to go !\n",
            "Let me see : I ' ll give them a new pair of boots every Christmas .'\n",
            "And she went on planning to herself how she would manage it .\n",
            "' They must go by the carrier ,' she thought ; ' and how funny it ' ll seem , sending presents to one ' s own feet !\n",
            "And how odd the directions will look !\n",
            "ALICE ' S RIGHT FOOT , ESQ .\n",
            "HEARTHRUG , NEAR THE FENDER , ( WITH ALICE ' S LOVE ).\n",
            "Oh dear , what nonsense I ' m talking !'\n",
            "Just then her head struck against the roof of the hall : in fact she was now more than nine feet high , and she at once took up the little golden key and hurried off to the garden door .\n",
            "Poor Alice !\n",
            "It was as much as she could do , lying down on one side , to look through into the garden with one eye ; but to get through was more hopeless than ever : she sat down and began to cry again .\n",
            "' You ought to be ashamed of yourself ,' said Alice , ' a great girl like you ,' ( she might well say this ), ' to go on crying in this way !\n",
            "Stop this moment , I tell you !'\n",
            "But she went on all the same , shedding gallons of tears , until there was a large pool all round her , about four inches deep and reaching half down the hall .\n",
            "After a time she heard a little pattering of feet in the distance , and she hastily dried her eyes to see what was coming .\n",
            "It was the White Rabbit returning , splendidly dressed , with a pair of white kid gloves in one hand and a large fan in the other : he came trotting along in a great hurry , muttering to himself as he came , ' Oh !\n",
            "the Duchess , the Duchess !\n",
            "Oh !\n",
            "won ' t she be savage if I ' ve kept her waiting !'\n",
            "Alice felt so desperate that she was ready to ask help of any one ; so , when the Rabbit came near her , she began , in a low , timid voice , ' If you please , sir --' The Rabbit started violently , dropped the white kid gloves and the fan , and skurried away into the darkness as hard as he could go .\n",
            "Alice took up the fan and gloves , and , as the hall was very hot , she kept fanning herself all the time she went on talking : ' Dear , dear !\n",
            "How queer everything is to - day !\n",
            "And yesterday things went on just as usual .\n",
            "I wonder if I ' ve been changed in the night ?\n",
            "Let me think : was I the same when I got up this morning ?\n",
            "I almost think I can remember feeling a little different .\n",
            "But if I ' m not the same , the next question is , Who in the world am I ?\n",
            "Ah , THAT ' S the great puzzle !'\n",
            "And she began thinking over all the children she knew that were of the same age as herself , to see if she could have been changed for any of them .\n",
            "' I ' m sure I ' m not Ada ,' she said , ' for her hair goes in such long ringlets , and mine doesn ' t go in ringlets at all ; and I ' m sure I can ' t be Mabel , for I know all sorts of things , and she , oh !\n",
            "she knows such a very little !\n",
            "Besides , SHE ' S she , and I ' m I , and -- oh dear , how puzzling it all is !\n",
            "I ' ll try if I know all the things I used to know .\n",
            "Let me see : four times five is twelve , and four times six is thirteen , and four times seven is -- oh dear !\n",
            "I shall never get to twenty at that rate !\n",
            "However , the Multiplication Table doesn ' t signify : let ' s try Geography .\n",
            "London is the capital of Paris , and Paris is the capital of Rome , and Rome -- no , THAT ' S all wrong , I ' m certain !\n",
            "I must have been changed for Mabel !\n",
            "I ' ll try and say \" How doth the little --\"' and she crossed her hands on her lap as if she were saying lessons , and began to repeat it , but her voice sounded hoarse and strange , and the words did not come the same as they used to do :--\n",
            "' How doth the little crocodile Improve his shining tail , And pour the waters of the Nile On every golden scale !\n",
            "' How cheerfully he seems to grin , How neatly spread his claws , And welcome little fishes in With gently smiling jaws !'\n",
            "' I ' m sure those are not the right words ,' said poor Alice , and her eyes filled with tears again as she went on , ' I must be Mabel after all , and I shall have to go and live in that poky little house , and have next to no toys to play with , and oh !\n",
            "ever so many lessons to learn !\n",
            "No , I ' ve made up my mind about it ; if I ' m Mabel , I ' ll stay down here !\n",
            "It ' ll be no use their putting their heads down and saying \" Come up again , dear !\"\n",
            "I shall only look up and say \" Who am I then ?\n",
            "Tell me that first , and then , if I like being that person , I ' ll come up : if not , I ' ll stay down here till I ' m somebody else \"-- but , oh dear !'\n",
            "cried Alice , with a sudden burst of tears , ' I do wish they WOULD put their heads down !\n",
            "I am so VERY tired of being all alone here !'\n",
            "As she said this she looked down at her hands , and was surprised to see that she had put on one of the Rabbit ' s little white kid gloves while she was talking .\n",
            "' How CAN I have done that ?'\n",
            "she thought .\n",
            "' I must be growing small again .'\n",
            "She got up and went to the table to measure herself by it , and found that , as nearly as she could guess , she was now about two feet high , and was going on shrinking rapidly : she soon found out that the cause of this was the fan she was holding , and she dropped it hastily , just in time to avoid shrinking away altogether .\n",
            "' That WAS a narrow escape !'\n",
            "said Alice , a good deal frightened at the sudden change , but very glad to find herself still in existence ; ' and now for the garden !'\n",
            "and she ran with all speed back to the little door : but , alas !\n",
            "the little door was shut again , and the little golden key was lying on the glass table as before , ' and things are worse than ever ,' thought the poor child , ' for I never was so small as this before , never !\n",
            "And I declare it ' s too bad , that it is !'\n",
            "As she said these words her foot slipped , and in another moment , splash !\n",
            "she was up to her chin in salt water .\n",
            "Her first idea was that she had somehow fallen into the sea , ' and in that case I can go back by railway ,' she said to herself .\n",
            "( Alice had been to the seaside once in her life , and had come to the general conclusion , that wherever you go to on the English coast you find a number of bathing machines in the sea , some children digging in the sand with wooden spades , then a row of lodging houses , and behind them a railway station .)\n",
            "However , she soon made out that she was in the pool of tears which she had wept when she was nine feet high .\n",
            "' I wish I hadn ' t cried so much !'\n",
            "said Alice , as she swam about , trying to find her way out .\n",
            "' I shall be punished for it now , I suppose , by being drowned in my own tears !\n",
            "That WILL be a queer thing , to be sure !\n",
            "However , everything is queer to - day .'\n",
            "Just then she heard something splashing about in the pool a little way off , and she swam nearer to make out what it was : at first she thought it must be a walrus or hippopotamus , but then she remembered how small she was now , and she soon made out that it was only a mouse that had slipped in like herself .\n",
            "' Would it be of any use , now ,' thought Alice , ' to speak to this mouse ?\n",
            "Everything is so out - of - the - way down here , that I should think very likely it can talk : at any rate , there ' s no harm in trying .'\n",
            "So she began : ' O Mouse , do you know the way out of this pool ?\n",
            "I am very tired of swimming about here , O Mouse !'\n",
            "( Alice thought this must be the right way of speaking to a mouse : she had never done such a thing before , but she remembered having seen in her brother ' s Latin Grammar , ' A mouse -- of a mouse -- to a mouse -- a mouse -- O mouse !')\n",
            "The Mouse looked at her rather inquisitively , and seemed to her to wink with one of its little eyes , but it said nothing .\n",
            "' Perhaps it doesn ' t understand English ,' thought Alice ; ' I daresay it ' s a French mouse , come over with William the Conqueror .'\n",
            "( For , with all her knowledge of history , Alice had no very clear notion how long ago anything had happened .)\n",
            "So she began again : ' Ou est ma chatte ?'\n",
            "which was the first sentence in her French lesson - book .\n",
            "The Mouse gave a sudden leap out of the water , and seemed to quiver all over with fright .\n",
            "' Oh , I beg your pardon !'\n",
            "cried Alice hastily , afraid that she had hurt the poor animal ' s feelings .\n",
            "' I quite forgot you didn ' t like cats .'\n",
            "' Not like cats !'\n",
            "cried the Mouse , in a shrill , passionate voice .\n",
            "' Would YOU like cats if you were me ?'\n",
            "' Well , perhaps not ,' said Alice in a soothing tone : ' don ' t be angry about it .\n",
            "And yet I wish I could show you our cat Dinah : I think you ' d take a fancy to cats if you could only see her .\n",
            "She is such a dear quiet thing ,' Alice went on , half to herself , as she swam lazily about in the pool , ' and she sits purring so nicely by the fire , licking her paws and washing her face -- and she is such a nice soft thing to nurse -- and she ' s such a capital one for catching mice -- oh , I beg your pardon !'\n",
            "cried Alice again , for this time the Mouse was bristling all over , and she felt certain it must be really offended .\n",
            "' We won ' t talk about her any more if you ' d rather not .'\n",
            "' We indeed !'\n",
            "cried the Mouse , who was trembling down to the end of his tail .\n",
            "' As if I would talk on such a subject !\n",
            "Our family always HATED cats : nasty , low , vulgar things !\n",
            "Don ' t let me hear the name again !'\n",
            "' I won ' t indeed !'\n",
            "said Alice , in a great hurry to change the subject of conversation .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU5U3bKnYknH",
        "colab_type": "text"
      },
      "source": [
        "#  NGrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBSWaZr_lKCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from difflib import SequenceMatcher \n",
        "\n",
        "\n",
        "class NgramModel():\n",
        "  def __init__(self,ngram_size,chars=False):\n",
        "    self.ngram_size = ngram_size\n",
        "    self.chars=chars\n",
        "    \n",
        "  def fit(self,text):\n",
        "    ngram_counter = Counter(self._ngrams(text))\n",
        "    self.ngram_counter = {ngram: count for ngram, count in zip(ngram_counter, self._normalise(ngram_counter.values()))}\n",
        "              \n",
        "  def generate(self, words, iterations=30, k_beams = 3, diversity_threshold = .9):\n",
        "    beams = [(1., words, self._ngrams(words)[-1])]\n",
        "    for _ in range(iterations):\n",
        "      #first we score next for each beam (based on frequency & similarity of ngrams) and keep the best k\n",
        "      candidates = []\n",
        "      for p, ws, prev_ngram in beams:\n",
        "        prev_ngram = ' '.join(prev_ngram.split()[1:])\n",
        "        new_candidates = [(p + f + self._compare(prev_ngram, ' '.join(ngram.split()[:-1]) ) , ws + [' ',''][self.chars] + ngram.split()[-1], ngram ) for ngram,f in self.ngram_counter.items() ] \n",
        "        new_candidates = sorted(new_candidates,reverse=True)[:k_beams]                \n",
        "        candidates.extend(new_candidates)\n",
        "      #now we filter out any beams that are too similar to other beams (to keep diversity)\n",
        "      candidates = sorted(candidates,reverse=True)\n",
        "      beams = [candidates[0]]\n",
        "      for p,ws,ng in candidates[1:]:\n",
        "        similarity = max([self._compare(ws,ws2) for _,ws2,_ in beams])\n",
        "        if similarity > diversity_threshold:\n",
        "          p = 0\n",
        "        beams.append((p,ws,ng))\n",
        "      #now we keep only the top k beams (for memory efficiency)\n",
        "      beams = sorted(beams, reverse=True)[:k_beams]\n",
        "    return [ws.replace('_',' ') for _,ws,_ in beams]\n",
        "       \n",
        "  def _tokenise(self,text):\n",
        "    return ''.join([c if ord('a') <= ord(c) <= ord('z') else f' {c} ' for c in text.lower()]).split()\n",
        "  \n",
        "  def _ngrams(self,text):\n",
        "      tokens = self._tokenise(text)\n",
        "      if self.chars: #char-level ngrams\n",
        "        tokens = list(' '.join(tokens).replace(' ','_'))\n",
        "      ngrams = zip(*[tokens[i:] for i in range(self.ngram_size)])\n",
        "      return [\" \".join(ngram) for ngram in ngrams]\n",
        "  \n",
        "  def _normalise(self, X):\n",
        "    s = sum(X)\n",
        "    return [x/s for x in X]\n",
        "  \n",
        "  def _compare(self, string1,string2):\n",
        "    return SequenceMatcher(None,string1, string2).ratio()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM27B3yW_q1c",
        "colab_type": "code",
        "outputId": "35a5936d-c502-43b4-f447-1a73e234d742",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "ng = NgramModel(4)\n",
        "ng.fit(text)\n",
        "ng.generate(\"she wanted to go\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"she wanted to go ! let me see : i ' ll try if i know all the things i used to know . let me see : i ' ll try if i\",\n",
              " 'she wanted to go ! let me think : was i the same when i got up this morning ? i almost think i can remember feeling a little different . but if i',\n",
              " 'she wanted to go and live in that poky little house , and have next to no toys to play with , and oh ! ever so many lessons to learn ! no ,']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipf-3lS7twzp",
        "colab_type": "code",
        "outputId": "ce7268aa-0ab9-445d-ef7a-8fff0743a243",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "ng = NgramModel(4, chars=True)\n",
        "ng.fit(text)\n",
        "ng.generate(\"she wanted to go\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['she wanted to go be a mouse , and she said ali',\n",
              " 'she wanted to golden she said alice , and she ',\n",
              " 'she wanted to got , the said alice , and she s']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FStxf0fri0Z",
        "colab_type": "text"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVNg3xs3izpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import keras\n",
        "\n",
        "class LSTMmodel():\n",
        "  \n",
        "  def fit(self,text,epochs=100):\n",
        "    self._load(text)\n",
        "    self._build()\n",
        "    self._train(epochs)\n",
        "\n",
        "  def generate(self,words,i=30,top_k = 0):\n",
        "    for _ in range(i):\n",
        "      x = [self.token_idx[token] if token in self.token_idx else 1 for token in self._tokenise(words)] \n",
        "      x = keras.preprocessing.sequence.pad_sequences([x], maxlen=self.x_dim, padding = 'pre')\n",
        "      if top_k:\n",
        "        probabilities = self.model.predict(x)[0]\n",
        "        ips = [(p,i) for i,p in enumerate(probabilities)]\n",
        "        top_candidates = sorted(ips,reverse=True)[:top_k]\n",
        "        probabilities = [p for p,_ in top_candidates]\n",
        "        s = sum(probabilities)\n",
        "        y_hat = np.random.choice([i for _,i in top_candidates], p=[prob/s for prob in probabilities]) #randomise\n",
        "      else:\n",
        "        y_hat = self.model.predict_classes(x, verbose=0)[0] #maximise\n",
        "      words += ' ' + self.idx_token[y_hat]\n",
        "    return words\n",
        "  \n",
        "  def _tokenise(self,text):\n",
        "    return ''.join([c if ord('a') <= ord(c) <= ord('z') else f' {c} ' for c in text.lower()]).split()\n",
        "    \n",
        "  def _load(self, text):\n",
        "    self.idx_token = dict(enumerate(set(self._tokenise(text)),start=2))\n",
        "    self.idx_token[0] = '<PAD>'\n",
        "    self.idx_token[1] = '<UNK>' \n",
        "    self.token_idx = {word:i for i,word in self.idx_token.items()}       \n",
        "    token_ids = [[self.token_idx[token] for token in self._tokenise(sentence)] for sentence in text.split('\\n')]\n",
        "    inouts = [tokens[:i+1] for tokens in token_ids for i in range(1,len(tokens))]\n",
        "    self.x_dim = max([len(x) for x in inouts]) - 1\n",
        "    self.y_dim = len(self.idx_token) \n",
        "    inouts = np.array(keras.preprocessing.sequence.pad_sequences(inouts,maxlen=self.x_dim + 1, padding='pre'))\n",
        "    self.X, self.Y = inouts[:,:-1], inouts[:,-1]\n",
        "    \n",
        "  def _build(self):\n",
        "    self.model = keras.models.Sequential()\n",
        "    self.model.add(keras.layers.Embedding(self.y_dim, 10, input_length=self.x_dim))\n",
        "    self.model.add(keras.layers.LSTM(150, return_sequences = True))\n",
        "    self.model.add(keras.layers.LSTM(100))\n",
        "    self.model.add(keras.layers.Dense(self.y_dim, activation='softmax'))\n",
        "    self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "  def _train(self,epochs):\n",
        "    earlystop =  keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "    onehot_y = keras.utils.to_categorical(self.Y, num_classes=self.y_dim)\n",
        "    self.model.fit(self.X, onehot_y, epochs=epochs, verbose=1, callbacks=[earlystop])  \n",
        "  \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqaJLiJHsE3g",
        "colab_type": "code",
        "outputId": "42c1fa97-3f5c-45d8-afd1-fc966d18564b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3526
        }
      },
      "source": [
        "lstm = LSTMmodel()\n",
        "lstm.fit(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "2186/2186 [==============================] - 22s 10ms/step - loss: 5.7124 - acc: 0.0462\n",
            "Epoch 2/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2186/2186 [==============================] - 19s 9ms/step - loss: 5.3298 - acc: 0.0682\n",
            "Epoch 3/100\n",
            "2186/2186 [==============================] - 19s 9ms/step - loss: 5.3023 - acc: 0.0631\n",
            "Epoch 4/100\n",
            "2186/2186 [==============================] - 19s 9ms/step - loss: 5.2955 - acc: 0.0682\n",
            "Epoch 5/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2904 - acc: 0.0613\n",
            "Epoch 6/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2894 - acc: 0.0640\n",
            "Epoch 7/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2884 - acc: 0.0682\n",
            "Epoch 8/100\n",
            "2186/2186 [==============================] - 19s 9ms/step - loss: 5.2889 - acc: 0.0654\n",
            "Epoch 9/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2900 - acc: 0.0650\n",
            "Epoch 10/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2883 - acc: 0.0682\n",
            "Epoch 11/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2866 - acc: 0.0650\n",
            "Epoch 12/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2875 - acc: 0.0682\n",
            "Epoch 13/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2871 - acc: 0.0682\n",
            "Epoch 14/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2893 - acc: 0.0682\n",
            "Epoch 15/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2881 - acc: 0.0682\n",
            "Epoch 16/100\n",
            "2186/2186 [==============================] - 19s 9ms/step - loss: 5.2874 - acc: 0.0682\n",
            "Epoch 17/100\n",
            "2186/2186 [==============================] - 19s 9ms/step - loss: 5.2878 - acc: 0.0682\n",
            "Epoch 18/100\n",
            "2186/2186 [==============================] - 19s 9ms/step - loss: 5.2893 - acc: 0.0682\n",
            "Epoch 19/100\n",
            "2186/2186 [==============================] - 19s 9ms/step - loss: 5.2878 - acc: 0.0627\n",
            "Epoch 20/100\n",
            "2186/2186 [==============================] - 19s 9ms/step - loss: 5.2874 - acc: 0.0682\n",
            "Epoch 21/100\n",
            "2186/2186 [==============================] - 19s 9ms/step - loss: 5.2873 - acc: 0.0640\n",
            "Epoch 22/100\n",
            "2186/2186 [==============================] - 19s 9ms/step - loss: 5.2854 - acc: 0.0640\n",
            "Epoch 23/100\n",
            "2186/2186 [==============================] - 19s 9ms/step - loss: 5.2880 - acc: 0.0682\n",
            "Epoch 24/100\n",
            "2186/2186 [==============================] - 19s 9ms/step - loss: 5.2872 - acc: 0.0631\n",
            "Epoch 25/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2862 - acc: 0.0645\n",
            "Epoch 26/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2900 - acc: 0.0659\n",
            "Epoch 27/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2877 - acc: 0.0682\n",
            "Epoch 28/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2841 - acc: 0.0682\n",
            "Epoch 29/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2849 - acc: 0.0682\n",
            "Epoch 30/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2868 - acc: 0.0668\n",
            "Epoch 31/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2869 - acc: 0.0650\n",
            "Epoch 32/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2894 - acc: 0.0682\n",
            "Epoch 33/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2909 - acc: 0.0682\n",
            "Epoch 34/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2862 - acc: 0.0682\n",
            "Epoch 35/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2847 - acc: 0.0682\n",
            "Epoch 36/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2879 - acc: 0.0682\n",
            "Epoch 37/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2867 - acc: 0.0682\n",
            "Epoch 38/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2873 - acc: 0.0682\n",
            "Epoch 39/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2876 - acc: 0.0682\n",
            "Epoch 40/100\n",
            "2186/2186 [==============================] - 19s 9ms/step - loss: 5.2876 - acc: 0.0682\n",
            "Epoch 41/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2861 - acc: 0.0682\n",
            "Epoch 42/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2854 - acc: 0.0682\n",
            "Epoch 43/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2876 - acc: 0.0682\n",
            "Epoch 44/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2866 - acc: 0.0682\n",
            "Epoch 45/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2890 - acc: 0.0682\n",
            "Epoch 46/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2888 - acc: 0.0622\n",
            "Epoch 47/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2880 - acc: 0.0686\n",
            "Epoch 48/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2874 - acc: 0.0682\n",
            "Epoch 49/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2873 - acc: 0.0682\n",
            "Epoch 50/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2889 - acc: 0.0682\n",
            "Epoch 51/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2863 - acc: 0.0682\n",
            "Epoch 52/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2876 - acc: 0.0682\n",
            "Epoch 53/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2876 - acc: 0.0636\n",
            "Epoch 54/100\n",
            "2186/2186 [==============================] - 19s 9ms/step - loss: 5.2873 - acc: 0.0682\n",
            "Epoch 55/100\n",
            "2186/2186 [==============================] - 19s 9ms/step - loss: 5.2895 - acc: 0.0640\n",
            "Epoch 56/100\n",
            "2186/2186 [==============================] - 19s 9ms/step - loss: 5.2865 - acc: 0.0682\n",
            "Epoch 57/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2904 - acc: 0.0631\n",
            "Epoch 58/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2883 - acc: 0.0682\n",
            "Epoch 59/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2891 - acc: 0.0622\n",
            "Epoch 60/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2855 - acc: 0.0682\n",
            "Epoch 61/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2853 - acc: 0.0682\n",
            "Epoch 62/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2846 - acc: 0.0668\n",
            "Epoch 63/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2883 - acc: 0.0650\n",
            "Epoch 64/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2880 - acc: 0.0682\n",
            "Epoch 65/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2880 - acc: 0.0682\n",
            "Epoch 66/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2897 - acc: 0.0682\n",
            "Epoch 67/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2891 - acc: 0.0650\n",
            "Epoch 68/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2884 - acc: 0.0682\n",
            "Epoch 69/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2909 - acc: 0.0682\n",
            "Epoch 70/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2897 - acc: 0.0682\n",
            "Epoch 71/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2893 - acc: 0.0682\n",
            "Epoch 72/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2866 - acc: 0.0682\n",
            "Epoch 73/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2867 - acc: 0.0682\n",
            "Epoch 74/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2910 - acc: 0.0682\n",
            "Epoch 75/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2874 - acc: 0.0682\n",
            "Epoch 76/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2870 - acc: 0.0682\n",
            "Epoch 77/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2914 - acc: 0.0654\n",
            "Epoch 78/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2861 - acc: 0.0677\n",
            "Epoch 79/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2906 - acc: 0.0622\n",
            "Epoch 80/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2897 - acc: 0.0618\n",
            "Epoch 81/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2893 - acc: 0.0631\n",
            "Epoch 82/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2897 - acc: 0.0682\n",
            "Epoch 83/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2888 - acc: 0.0682\n",
            "Epoch 84/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2870 - acc: 0.0636\n",
            "Epoch 85/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2851 - acc: 0.0682\n",
            "Epoch 86/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2869 - acc: 0.0682\n",
            "Epoch 87/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2843 - acc: 0.0682\n",
            "Epoch 88/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2881 - acc: 0.0682\n",
            "Epoch 89/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2877 - acc: 0.0682\n",
            "Epoch 90/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2887 - acc: 0.0627\n",
            "Epoch 91/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2874 - acc: 0.0682\n",
            "Epoch 92/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2860 - acc: 0.0682\n",
            "Epoch 93/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2874 - acc: 0.0622\n",
            "Epoch 94/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2896 - acc: 0.0682\n",
            "Epoch 95/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2885 - acc: 0.0682\n",
            "Epoch 96/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2886 - acc: 0.0682\n",
            "Epoch 97/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2861 - acc: 0.0650\n",
            "Epoch 98/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2907 - acc: 0.0682\n",
            "Epoch 99/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2865 - acc: 0.0636\n",
            "Epoch 100/100\n",
            "2186/2186 [==============================] - 20s 9ms/step - loss: 5.2915 - acc: 0.0631\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-dsu1Tj_BJi",
        "colab_type": "code",
        "outputId": "2f63c18c-f8d3-4a19-9c86-f9560415385f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(lstm.generate(seed))\n",
        "print(lstm.generate(seed, top_k=3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "what , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
            "what , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHRcwZeFIRzo",
        "colab_type": "text"
      },
      "source": [
        "## ULMFIT (AWD-LSTM & Transfer Learning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OiEaAFApkwa",
        "colab_type": "code",
        "outputId": "c9fa4d36-1cdd-41e8-9c74-77d208444d1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "training_data = [sentence.split()[:i+1] for sentence in text.split('\\n') for i in range(1,len(sentence.split()))]\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "    \"text\":[' '.join(sample[:-1]) for sample in training_data],\n",
        "    \"label\":[sample[-1] for sample in training_data],\n",
        "})\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I</td>\n",
              "      <td>'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I '</td>\n",
              "      <td>m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I ' m</td>\n",
              "      <td>sure</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I ' m sure</td>\n",
              "      <td>_I_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I ' m sure _I_</td>\n",
              "      <td>shan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             text label\n",
              "0               I     '\n",
              "1             I '     m\n",
              "2           I ' m  sure\n",
              "3      I ' m sure   _I_\n",
              "4  I ' m sure _I_  shan"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ0X-CZWq04G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.text import language_model_learner, TextLMDataBunch,untar_data,URLs,AWD_LSTM\n",
        "ulmfit = language_model_learner(TextLMDataBunch.from_df(untar_data(URLs.IMDB_SAMPLE),train_df = df , valid_df = df ), AWD_LSTM, drop_mult=.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWmkqFwfOh01",
        "colab_type": "code",
        "outputId": "08b8cdec-d3fd-4bfb-fd3d-11f9ca1106bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "ulmfit.predict(seed, n_words=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'..how was a large - and - large , - off , - Mouse : a ) being over - used when it was found out . The Use Was Sudden That ( a ) was a French by - hands , used to be used for the Rome Times and French Times . In \" The Great Can See , \" the four - used Through The Door of the Being was a French Just'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Kb_kmSzm4IN",
        "colab_type": "code",
        "outputId": "68774fc6-e4e1-4b3b-e3f3-32df4f22ded5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        }
      },
      "source": [
        "ulmfit.fit_one_cycle(1, 1e-2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>7.214114</td>\n",
              "      <td>6.888145</td>\n",
              "      <td>0.014286</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tJNZoX-rAPN",
        "colab_type": "code",
        "outputId": "fba3a6a7-0ab0-438d-c6bf-f5b9f034d322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "ulmfit.predict(seed, n_words=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'..how was in Paris when \" There was a French - French No - Get Me ! right there to see ! \" , he said , \" when i was in the back of the Sea of , i \\' m You i \\' ve Your Eyes \" . The French and English Times They Are a Of The Times , The Times \\' Talk OF The Times ,'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBC92HQjRL4c",
        "colab_type": "text"
      },
      "source": [
        "# Char-Level LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PfJTHiIRP9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import keras\n",
        "\n",
        "class charLSTMmodel():\n",
        "  \n",
        "  def fit(self,text,epochs=100):\n",
        "    self._load(text)\n",
        "    self._build()\n",
        "    self._train(epochs)\n",
        "\n",
        "  def generate(self,words,i=150):\n",
        "    for _ in range(i):\n",
        "      x = [self.token_idx[token] if token in self.token_idx else 1 for token in self._tokenise(words)] \n",
        "      x = keras.preprocessing.sequence.pad_sequences([x], maxlen=self.x_dim, padding = 'pre')\n",
        "      y_hat = self.model.predict_classes(x, verbose=0)[0] #maximise\n",
        "      words += self.idx_token[y_hat]\n",
        "    return words.replace(\"_\",\" \")\n",
        "  \n",
        "  def _chunk(self,text,chunk_size = 100):\n",
        "    return ''.join([c + '<S>' if not i % chunk_size else c for i,c in enumerate(text,start=1)]).split('<S>')\n",
        "  \n",
        "  def _tokenise(self,text):\n",
        "    return list(' '.join(text.split()).replace(\" \",\"_\"))\n",
        "    \n",
        "  def _load(self, text):\n",
        "    self.idx_token = dict(enumerate(set(self._tokenise(text)),start=2))\n",
        "    self.idx_token[0] = '<PAD>'\n",
        "    self.idx_token[1] = '<UNK>' \n",
        "    self.token_idx = {word:i for i,word in self.idx_token.items()}       \n",
        "    token_ids = [[self.token_idx[token] for token in self._tokenise(sentence)] for sentence in self._chunk(text)]\n",
        "    inouts = [tokens[:i+1] for tokens in token_ids for i in range(1,len(tokens))]\n",
        "    self.x_dim = max([len(x) for x in inouts]) - 1\n",
        "    self.y_dim = len(self.idx_token) \n",
        "    inouts = np.array(keras.preprocessing.sequence.pad_sequences(inouts,maxlen=self.x_dim + 1, padding='pre'))\n",
        "    self.X, self.Y = inouts[:,:-1], inouts[:,-1]\n",
        "    \n",
        "  def _build(self):\n",
        "    self.model = keras.models.Sequential()\n",
        "    self.model.add(keras.layers.Embedding(self.y_dim, 10, input_length=self.x_dim))\n",
        "    self.model.add(keras.layers.LSTM(150, return_sequences = True))\n",
        "    self.model.add(keras.layers.LSTM(100))\n",
        "    self.model.add(keras.layers.Dense(self.y_dim, activation='softmax'))\n",
        "    self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "  def _train(self,epochs):\n",
        "    earlystop =  keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "    onehot_y = keras.utils.to_categorical(self.Y, num_classes=self.y_dim)\n",
        "    self.model.fit(self.X, onehot_y, epochs=epochs, verbose=1, callbacks=[earlystop])  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbr1RKGJTAPt",
        "colab_type": "code",
        "outputId": "05cf7faa-bcf9-412e-e6ea-e46ceb965e88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1799
        }
      },
      "source": [
        "clstm = charLSTMmodel()\n",
        "clstm.fit(text, epochs=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "9379/9379 [==============================] - 107s 11ms/step - loss: 3.0769 - acc: 0.2306\n",
            "Epoch 2/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "9379/9379 [==============================] - 105s 11ms/step - loss: 2.9091 - acc: 0.2452\n",
            "Epoch 3/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 2.6316 - acc: 0.2919\n",
            "Epoch 4/50\n",
            "9379/9379 [==============================] - 104s 11ms/step - loss: 2.4382 - acc: 0.3276\n",
            "Epoch 5/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 2.7868 - acc: 0.2730\n",
            "Epoch 6/50\n",
            "9379/9379 [==============================] - 104s 11ms/step - loss: 2.4019 - acc: 0.3371\n",
            "Epoch 7/50\n",
            "9379/9379 [==============================] - 104s 11ms/step - loss: 2.2483 - acc: 0.3688\n",
            "Epoch 8/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 2.1832 - acc: 0.3847\n",
            "Epoch 9/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 2.1257 - acc: 0.3983\n",
            "Epoch 10/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 2.0774 - acc: 0.4100\n",
            "Epoch 11/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 2.0267 - acc: 0.4235\n",
            "Epoch 12/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.9764 - acc: 0.4415\n",
            "Epoch 13/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.9335 - acc: 0.4540\n",
            "Epoch 14/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.8889 - acc: 0.4631\n",
            "Epoch 15/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.8349 - acc: 0.4762\n",
            "Epoch 16/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.8026 - acc: 0.4846\n",
            "Epoch 17/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.7388 - acc: 0.5009\n",
            "Epoch 18/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.6849 - acc: 0.5126\n",
            "Epoch 19/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.6236 - acc: 0.5332\n",
            "Epoch 20/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.5727 - acc: 0.5450\n",
            "Epoch 21/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.5135 - acc: 0.5654\n",
            "Epoch 22/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.4561 - acc: 0.5859\n",
            "Epoch 23/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.3944 - acc: 0.5986\n",
            "Epoch 24/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.3369 - acc: 0.6189\n",
            "Epoch 25/50\n",
            "9379/9379 [==============================] - 108s 11ms/step - loss: 1.2779 - acc: 0.6368\n",
            "Epoch 26/50\n",
            "9379/9379 [==============================] - 107s 11ms/step - loss: 1.2222 - acc: 0.6531\n",
            "Epoch 27/50\n",
            "9379/9379 [==============================] - 107s 11ms/step - loss: 1.1953 - acc: 0.6604\n",
            "Epoch 28/50\n",
            "9379/9379 [==============================] - 107s 11ms/step - loss: 1.1136 - acc: 0.6886\n",
            "Epoch 29/50\n",
            "9379/9379 [==============================] - 106s 11ms/step - loss: 1.0562 - acc: 0.7046\n",
            "Epoch 30/50\n",
            "9379/9379 [==============================] - 106s 11ms/step - loss: 0.9876 - acc: 0.7310\n",
            "Epoch 31/50\n",
            "9379/9379 [==============================] - 107s 11ms/step - loss: 0.9406 - acc: 0.7455\n",
            "Epoch 32/50\n",
            "9379/9379 [==============================] - 106s 11ms/step - loss: 0.8806 - acc: 0.7639\n",
            "Epoch 33/50\n",
            "9379/9379 [==============================] - 106s 11ms/step - loss: 0.8281 - acc: 0.7831\n",
            "Epoch 34/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.7728 - acc: 0.7978\n",
            "Epoch 35/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.7213 - acc: 0.8159\n",
            "Epoch 36/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.6710 - acc: 0.8337\n",
            "Epoch 37/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.6233 - acc: 0.8487\n",
            "Epoch 38/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.5783 - acc: 0.8612\n",
            "Epoch 39/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.5450 - acc: 0.8701\n",
            "Epoch 40/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.4916 - acc: 0.8884\n",
            "Epoch 41/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.4432 - acc: 0.9049\n",
            "Epoch 42/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.4108 - acc: 0.9113\n",
            "Epoch 43/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.3782 - acc: 0.9212\n",
            "Epoch 44/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.3434 - acc: 0.9328\n",
            "Epoch 45/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.3201 - acc: 0.9376\n",
            "Epoch 46/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.2767 - acc: 0.9508\n",
            "Epoch 47/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.2487 - acc: 0.9578\n",
            "Epoch 48/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.2315 - acc: 0.9615\n",
            "Epoch 49/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.2193 - acc: 0.9635\n",
            "Epoch 50/50\n",
            "9379/9379 [==============================] - 104s 11ms/step - loss: 0.1955 - acc: 0.9698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5SFStB8MZf7",
        "colab_type": "code",
        "outputId": "f808883a-cdd2-44d9-aa97-ba1c8954992e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "clstm.generate(\"indubitably \")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "indubitably elice , and her eyes filled with tears again . ' You ought to be ashamed of yourself ,' said Alice , ' to speak to this mouse , she was tomes on one h\n",
            "indubitably elice , and her eyes firled with tee whonged I wuber , spe bnowg a warden of yoor again . ' You ought to mriwpy and on the gool of any one , so gone ,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx7w7szoRTCn",
        "colab_type": "text"
      },
      "source": [
        "# char-LSTM with Attention & Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Pfybf0cnF-",
        "colab_type": "code",
        "outputId": "9ec77d6e-9020-4859-db51-f3b8b32954a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from textgenrnn import textgenrnn\n",
        "\n",
        "clstm_att = textgenrnn()\n",
        "clstm_att.generate()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[PS4] LF4M BOOSALIQ a promote of my computer at the mouse of the users made it to the stranger and then the shopping and all the best for the teammates. Thanks for a set of some possible shots at all time his boys on the entire popular decade and she didn't see an increase it would be transferent,\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-8pLZaUjEOq",
        "colab_type": "code",
        "outputId": "839abba4-fb0c-4025-a14b-db4250518d71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5115
        }
      },
      "source": [
        "clstm_att.train_on_texts(text.split('\\n'),num_epochs=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on 9,522 character sequences.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Epoch 1/10\n",
            "74/74 [==============================] - 29s 387ms/step - loss: 1.7788\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "' I had so the she she she she she was not to one of the she she she she she she she she she she was sure the here !'\n",
            "\n",
            "' S sure the she she she she she was surcement to she was sure to she she she she she she she was now in the sure the sure to when the she she she she she she she she was surportility to the she she she she she she she she she she she she was not the sure in the sure in the sure the sure it '  she\n",
            "\n",
            "' Alice '  ' she she she she she she she she she was not such a sure the she she she she she she she was now to the she she she she she she she she she she was not such a little she was now to such she was not such a little surcemen than the how her she was heards , and she was surced to such she \n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "' Rooo and said of the dicress to sure that she was the sure in door !'\n",
            "\n",
            "How to be able the vagin .\n",
            "\n",
            "' And she was such a be now to like and not the said of I want to the she he was on she was doenner , what she she she was have doen the good ; her way was surrives ' seng the begh and she herried , and she was beganded to the same to the planning if a be poor she had a come , '  !'\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Am some doou subiroment hall won don rally , of sunchaathed , engather again .'\n",
            "\n",
            "Howrine ; , '\n",
            "\n",
            "Alice use embart in her please sait , lawing sap hemald I house alten like to the great her queer kid rae into the house come hereveever , wake .'\n",
            "\n",
            "Epoch 2/10\n",
            "74/74 [==============================] - 23s 313ms/step - loss: 1.3146\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "' I 'm that she was some to her and she was been the she was a good garden , and she was not some , and she was the same of the same to her way on the sea , and she was a sea and she was to the she seemed to she she was such a she was to the garden is a she she was not such a sure and she was some\n",
            "\n",
            "' We can shall have her and she was her and she was not such a she was to the same that she was some the she was some all the same that the garden and she was some the garden , and she was some to the garden and she was saying to the same that she was such a she was not some all the same and she w\n",
            "\n",
            "' Alice is the sure and she she she was such a sure I was not a sure the same that she was saying the garden , and she was the sea and seemed to her any of the same that she was some the garden , and she was such a she was a good garden , and she was her to the she she was not some , and she was s\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "How Alice , and she was such a half singe in a she she was not some her on to half out the cat about really and she was said to her any the care , and she was had a back that not and she was said a distirnion , I was the little pool of she she she was not seen to see if you had her way it was such\n",
            "\n",
            "' Alice and she she swam her any eyes and she she went on the , and the garden , and she went on little fan , and she was dear , and she was she was said to the sour shand on her same that she went on see her it was all out the mooded for the hall alone was she was her some herself .\n",
            "\n",
            "Howe the way of her was up again , and she was began , and worse in the little seating is , and a golden , and she was every tone and she said to the first in the same that she swill the haur the garden I ' m say her little golden as the side of the carrors , and she was now , and hopel is some so\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "' way in her wade : '  ' l strupse by there , ah to build this bebound , and hama by to the very guite , but I have to tiegle favor in ye side , Williad ' geas and the Nate half only skill her is trying .\n",
            "\n",
            "she the puzzle heard hauchel better again : and hofe hever was I gloves on change , but what and !\n",
            "\n",
            "' Steaaly ' shall begg her she was cause be strict to had it a tuck you went on if she '  thing -- Than : '  O shall her becoverpot at her awessen away Fand a she senden to her if I was sure had we doed in stengle with plate , and funging withing , ' ( Long handrower fat : and I was hassed THI the\n",
            "\n",
            "Epoch 3/10\n",
            "74/74 [==============================] - 23s 313ms/step - loss: 1.1211\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "' Alice thing , and she she was now , and she was now , and she was now , and she she was now and she could say the first to the same to her way of the same to her any of the same and she was now , and she was now , and her she was not the same to she was to  her cat as her any of the same , and h\n",
            "\n",
            "' The Mouse , and she she was now , and she was now , and she she was now , and she was now , and she was the same and she was now , and she she was now to her dear , and she she said her had so her can she was now to her said the care , and she she she was now , and she was to one of the hurrier \n",
            "\n",
            "' We she she said a such a sure any of the same to her hall anytold she she said her said the subject , and she was such a sure and she was now , and she was now , and she was now , and she was now , and she she see she was now , and she she said , and she she said the same that think her she was \n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "clear \" when she soon had one of the , and she went on it , and she had sometome , as the sama , she was paying to any others , and she went on the way of the sea , ' t she said his sounder not the mouse , and her to the samay down herself , and she was such a low and she was to  her planning and \n",
            "\n",
            "' ' s a same and she could was such a long to think and she '  she '  she said Alice , and a sure about to the garden her said ' t said the darkness , and she she was now , she got to one of the book and she could say her any hard to a good really to the same to said her cats , and she she was suc\n",
            "\n",
            "' Mon she had to  and she was to  but that she was a longly as she was now for such a pair of her could say her cat and she she was a long of the darkness , and began , and she she was now , but the cat care , and she was to  I must be a good she was she could don ' s to one of it , and horse she \n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "( a mouse , LGLG Mouse , angly crest mesienhical cried the remarmana to tell of she had to  near a Taber , ' dar , ' let up go to ESO through aimass any she heard out than any way , she such a un again , ' hige offamily , her fier up this she want to her fact tears to pool as me to wood , and waot\n",
            "\n",
            "A be good perlioh againstly ,' a speed planning hauches !)\n",
            "\n",
            "she link as the cy about , said you ' long to not .\n",
            "\n",
            "Epoch 4/10\n",
            "74/74 [==============================] - 23s 315ms/step - loss: 0.9926\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "' We was her way of the Mouse , and she shall have to go to her hall .\n",
            "\n",
            "' We won ' t golden in the garden , and she said , and she was now , and she she soon , ' she said the Mouse , and the poor poor she she shad the fire , and she shall have had now , and she was not the poor sea , and she she she seems to go , and she had herself all the same again , and she was no\n",
            "\n",
            "' We won ' t shall had be a great hall ;-- and she shall have to go ; and she was not a great hall hastily , and she shall have to go to the same her of the same again , and she had be a great hall ;-- -- , and she was now , and she said the Mouse , and she had to  the same age and she was now , a\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "Adon Come out of the hurries , and she feet things , and neats , and she went on the same again , how do I '  the garden down shan nothing , and I '  more of the right way of the Rabbit and she she something to go  the garden , and she was so many to say she ssmort me found only about in the Mouse\n",
            "\n",
            "' I ' ll she she had shaven on the same that she can ' t her said the ' Is a great mut began to her all , and of the Mouse , now , and the Musical , and have has all of the cat again , and hastily , and she was only their hall the same have hoot on the poor indeed , I ' m beg saying speaking ,' sh\n",
            "\n",
            "' We she shall have beganding that she got the day of the cat looked , and side , and she '  must be a dear , and things , and have the little cats .\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "clear , who !\n",
            "\n",
            "twin I pass permanal retrient when has while she she said Alice , began how before , ' for must her mipsed heads , and mots togle , how lix ' voice hear in the hall .\n",
            "\n",
            "what would way asks : burst her fans : ' The ohh herself ; : all , and hurried you 'l a water padball saying had herself , during herself , a Masting ,' that speaking ?\n",
            "\n",
            "Epoch 5/10\n",
            "74/74 [==============================] - 23s 316ms/step - loss: 0.8890\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "' We she was speed to must be a great she had be a good her way offenden !\n",
            "\n",
            "I have to go on the same that she was to one of the same of things , and she had have been changed for the little for the little dear !\n",
            "\n",
            "' We won ' t she see something to the little sea things , and she was up thing that she was not the great her way out of the same and she was nothing that she was now , and she was not the garden !\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "cried the Mouse , and Alice , and the Rabbit seemed to speak that she seems to go that she could have been sand of the hall .\n",
            "\n",
            "I was trying to know the right thing !\n",
            "\n",
            "Try the Mouse is the sudden golden key !\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "I went on the Mons , and looking dooring rapidly am terry and she cass being about .\n",
            "\n",
            "But then you ' d oh !\n",
            "\n",
            "' How doth the starties White thing that would it shall should it !\n",
            "\n",
            "Epoch 6/10\n",
            "74/74 [==============================] - 23s 314ms/step - loss: 0.7942\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "I ' m sure I ' ll stay poor all the poor sech to the same and she was now , and she had now , ' s seemed to himself as he she said , and she said the times she said the times and she was now , and she said , and she said the subject , and she said the same age and she had be a good hall was all ov\n",
            "\n",
            "I ' m sure I ' m sure I ' ll to the same how she was now , and she was now , and she said the times and she said the same age and she she she see the same and she was now , and she was now , and she said , and she said , and she she said , and she said the subject , and she said , and she said , a\n",
            "\n",
            "' I ' m sure I ' m sure I ' m sure .\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "I smok the Mouse , and she shall my notice , do it : and she was to  and she had do : ' the hall was saided the poor dear , and she had to  now , and in a same how began , and she was now , and she was so think about her doesn't speed inches four little safe that she she see so the mouse , how lit\n",
            "\n",
            "Let me think : she looked all the same age into the same , and she she see me feet in the night , that way if I ' m sure ; but the little golden knowledgo are  hffarens , and she same to her fan and seemed to speak .\n",
            "\n",
            "I ' m sure I said the Mouse , and she sways the hall call and she could have been changed for the same , she was now , and she she sees the animation , and have his side , and she could go , and a little so now , and I must be a poor swin her centisu , and she began , and this time she she see s t\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "I could have been conclession !'\n",
            "\n",
            "( Alice are  any clittend the hauch and her phot and litton you ' t ann can come ,' thought if I ' lmm as varda much , s at her poor law words ' O suppose , and in one stow ' S beform ' s dinr kupped .\n",
            "\n",
            "she could go , all spades , and I '  more English ,' clear For , nast , and found out quiting himse .)\n",
            "\n",
            "Epoch 7/10\n",
            "74/74 [==============================] - 24s 318ms/step - loss: 0.7155\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "I ' ll try if I ' m sure I '  , and she was now , and she was now , and she was now , and she was now , and she was now , and she was now , and she was now , and she was now , the little golden key was such a little way , and she was now , and she was now , and she was now , and she went on , and \n",
            "\n",
            "' I won ' t she was to one of the Mouse , and hands , and she was took up a nice of the same age as she went on  With the pool of the same and she was now , and she was now , and she went on going on cried the cat to the glass to the same her way out of the sea , she went on , and she could do  , \n",
            "\n",
            "' We won ' t go and she seems to go to the glass and she went on the same age and she began , and she was now , and she was now , and she was now , and she was now , and she was now , and she went on , and she can see the same , and she was now , and she went on , and she was now , and she was now\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "As she was tight the little golden key in the same , and way off the way of this worse , that I ' l deplay to the poor sending carrier , and she was saying like this took a little way , and a same that were of the pool said Alice , ' , and welcome way .\n",
            "\n",
            "' I '  I shall be a great glad to find her any oh .\n",
            "\n",
            "' How doth the little dear : I think you ' look and she could go , and she was no try and she was now , and is so me and the garden was sure , and she went on , and she could go , ' she soon all she said , that walk how one of its little danch of the garden .\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "' Am I subjuch : Oh must about  Our fan in then , there of a tire familining she went on  growing not , and poor nar understand person .\n",
            "\n",
            "Stop , Ohher I can in a real leah to all she began in this , hope it .\n",
            "\n",
            "' Ahabout bathed , the White Rome cated on concrisions !\n",
            "\n",
            "Epoch 8/10\n",
            "74/74 [==============================] - 24s 329ms/step - loss: 0.6530\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "' We see if I ' m sure I ' ll stay cats , and she was now , and she was now , and she was now , and she went on child , and she was now , and she was now , and she was now , and she was now , and she went on changed again , and she said , and she , ' t she soon and she was now about the first sent\n",
            "\n",
            "I ' m sure I ' ll try in the same , and she was now , and she went on such a narrow and she was now , and she was now , and she went on changed again , and she was digging as he same the poor cat to the same age and she was now about to the same age and she said the little different .\n",
            "\n",
            "I ' m sure I ' m sure _I_ I ' m sure ; I ' ll stay poor animu to the little wgo do it , and she was now , and she was now , and she went on , and she was now , and she was now , and she went on , and she was digging about in the same age and she was now , and she went on the same , and she went on\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "And she went on the same again was the first poor send and see if I ' ll try in a splame , and was to look then the poor cats !\n",
            "\n",
            "' I '  she said these she seems to go by out that it had come trotting and she remembered as he was rather spreainched her , and she went on the time she can dear , and not , she went on , ' O mouse !'\n",
            "\n",
            "she was now !\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "However , at she began , I '  before : but nothing .\n",
            "\n",
            "Send , so mutterial to speak , and she themsions which if yourself is on in existence , ' I have been changed with that wear of all !\n",
            "\n",
            "' Aham Youtubers , I Frese Ice get that ,avey cybed the garden , I ' ll or she could have behavas .'\n",
            "\n",
            "Epoch 9/10\n",
            "74/74 [==============================] - 23s 317ms/step - loss: 0.5864\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "' We do the Mouse -- I ' ll stay down here , and she said the times and she was now , and she was now about to the garden !\n",
            "\n",
            "' We won ' t she see so think you '  she soon she was to go back to the little deal in the pool , and she was now , and she was now , and she was now , and she was now , and she said , and she was such a shrill , and she was now , and she was now , and she said , and she said the try and she began\n",
            "\n",
            "' I won ' t she see something she she was to  the same had happened .\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "' I ' ll seem of as she was to go .\n",
            "\n",
            "And she was so fan to speak , who was triped as he said these , and she will do : \n",
            "\n",
            "I shall be the fire , and she began : in the same when she leap her had not all she had to  out of the time she could have been  , and she was now about it !\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "But then a right is this upseper grint !\n",
            "\n",
            "Perhare not O VEN - We not see become forgot this that way you have !\n",
            "\n",
            "the pool of the Rabbit , and there you had begron hot perht as she won doesn ' t just to  much the care about herself , sheen to speak house .)\n",
            "\n",
            "Epoch 10/10\n",
            "74/74 [==============================] - 24s 319ms/step - loss: 0.5402\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "' I ' m sure I ' ll try in the same age and she was such a sea and water and she was now , and she was now , and she went on the garden , and she was such a sea and was the first sentile , and she said the times again , and she said to say the subject of things was trying to go to the first side o\n",
            "\n",
            "' We do the Mouse was been changed all of his children staying , and I ' m sure I ' ll stay goes to her hands , and she shall have been changed for the first sentence , and she , ' s a sudden change !'\n",
            "\n",
            "' I won ' t be a great great she came : ' the little door was so much as he was now about here , and she seems to go back to the little door : she said the try to the same here all she went on talking !\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "As she went on she knew that I must be worn while she was now , the little golden key of the right , and was wish that side me can the same that she was to go on come ago at all .\n",
            "\n",
            "But will her began with to the little law you '  said Alice , and she hastily , and the fan is thirteing to all over four waiting !'\n",
            "\n",
            "Alice talking to the subject was took up the same of the same , and in the pool !\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Burly she heard near , and the poor of shall have hngp his pridilinci.\n",
            "\n",
            "Pard on I alt how , this nothing had have beganding she wherever !\n",
            "\n",
            "as she wons to fide thing if I 'm m !'\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibZY8T0fiERZ",
        "colab_type": "code",
        "outputId": "df40afc5-e49a-4d12-b3f0-f57268ab0c84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "clstm_att.generate()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Mouse was not as the time she was to one of estist if I ' more , and began to find me , I ' ll male at all the try she stops down the same again .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}