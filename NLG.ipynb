{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLG.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammedterry/NLP_for_ML/blob/master/NLG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MV3k0ORgkOz",
        "colab_type": "text"
      },
      "source": [
        "# GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8INhHKGgnJb",
        "colab_type": "code",
        "outputId": "790069e3-0d7d-4e8f-f253-f9d7a7823f17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "!git clone https://github.com/openai/gpt-2\n",
        "import os\n",
        "os.chdir('gpt-2')\n",
        "!pip3 install -r requirements.txt\n",
        "!python3 download_model.py 345M"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 174, done.\u001b[K\n",
            "remote: Total 174 (delta 0), reused 0 (delta 0), pack-reused 174\u001b[K\n",
            "Receiving objects: 100% (174/174), 4.35 MiB | 8.41 MiB/s, done.\n",
            "Resolving deltas: 100% (90/90), done.\n",
            "Requirement already satisfied: fire>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.1.3)\n",
            "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (2017.4.5)\n",
            "Requirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Requirement already satisfied: tqdm==4.31.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.31.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Fetching checkpoint: 1.00kit [00:00, 654kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 48.4Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 647kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:29, 48.1Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 5.64Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 41.9Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 39.1Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX21RRRtVaXv",
        "colab_type": "code",
        "outputId": "7a37d2ef-ca51-482c-d9a2-88170b9f9a4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        }
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --model_name=345M --nsamples=3 --length=30 --top_k 40 --temperature 0.7"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-14 00:23:41.181344: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-05-14 00:23:41.181822: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2f5d080 executing computations on platform Host. Devices:\n",
            "2019-05-14 00:23:41.181877: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/gpt-2/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/gpt-2/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2019-05-14 00:23:51.043804: W tensorflow/core/framework/allocator.cc:124] Allocation of 205852672 exceeds 10% of system memory.\n",
            "Model prompt >>> curiouser and curiouser\n",
            "======================================== SAMPLE 1 ========================================\n",
            ", the more I'm sure it's a good thing. That's because the problem with the story is that it's not really about how bad it\n",
            "======================================== SAMPLE 2 ========================================\n",
            ", in the eyes of the world.\n",
            "\n",
            "And that's why the world, of course, is hungry for even more of this nonsense. And\n",
            "======================================== SAMPLE 3 ========================================\n",
            ", who could not, it was said, be ignorant of the truth, but had seen and heard it, and could make no other conclusion; and\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 68, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 86, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 127, in Fire\n",
            "    component_trace = _Fire(component, args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 366, in _Fire\n",
            "    component, remaining_args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 542, in _CallCallable\n",
            "    result = fn(*varargs, **kwargs)\n",
            "  File \"src/interactive_conditional_samples.py\", line 83, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1569, in __exit__\n",
            "    if exec_type is errors.OpError:\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz2kaKKK92Ls",
        "colab_type": "text"
      },
      "source": [
        "# Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XFYSeHW3cjm",
        "colab_type": "code",
        "outputId": "ec34b127-6501-4f3e-a776-520b5723c550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "sentences = gutenberg.sents('carroll-alice.txt') #'shakespeare-macbeth.txt') #"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1kpCGiK366Q",
        "colab_type": "code",
        "outputId": "2ca65a73-6523-45d5-f728-0569a0bb356b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "text = '\\n'.join([' '.join(sentence) for sentence in sentences])\n",
        "print(text[100:500])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e was beginning to get very tired of sitting by her sister on the bank , and of having nothing to do : once or twice she had peeped into the book her sister was reading , but it had no pictures or conversations in it , ' and what is the use of a book ,' thought Alice ' without pictures or conversation ?'\n",
            "So she was considering in her own mind ( as well as she could , for the hot day made her feel \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU5U3bKnYknH",
        "colab_type": "text"
      },
      "source": [
        "#  NGrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBSWaZr_lKCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from difflib import SequenceMatcher \n",
        "\n",
        "\n",
        "class NgramModel():\n",
        "  def __init__(self,ngram_size,chars=False):\n",
        "    self.ngram_size = ngram_size\n",
        "    self.chars=chars\n",
        "    \n",
        "  def fit(self,text):\n",
        "    ngram_counter = Counter(self._ngrams(text))\n",
        "    self.ngram_counter = {ngram: count for ngram, count in zip(ngram_counter, self._normalise(ngram_counter.values()))}\n",
        "              \n",
        "  def generate(self, words, iterations=30, k_beams = 3, diversity_threshold = .9):\n",
        "    beams = [(1., words, self._ngrams(words)[-1])]\n",
        "    for _ in range(iterations):\n",
        "      #first we score next for each beam (based on frequency & similarity of ngrams) and keep the best k\n",
        "      candidates = []\n",
        "      for p, ws, prev_ngram in beams:\n",
        "        prev_ngram = ' '.join(prev_ngram.split()[1:])\n",
        "        new_candidates = [(p + f + self._compare(prev_ngram, ' '.join(ngram.split()[:-1]) ) , ws + [' ',''][self.chars] + ngram.split()[-1], ngram ) for ngram,f in self.ngram_counter.items() ] \n",
        "        new_candidates = sorted(new_candidates,reverse=True)[:k_beams]                \n",
        "        candidates.extend(new_candidates)\n",
        "      #now we filter out any beams that are too similar to other beams (to keep diversity)\n",
        "      candidates = sorted(candidates,reverse=True)\n",
        "      beams = [candidates[0]]\n",
        "      for p,ws,ng in candidates[1:]:\n",
        "        similarity = max([self._compare(ws,ws2) for _,ws2,_ in beams])\n",
        "        if similarity > diversity_threshold:\n",
        "          p = 0\n",
        "        beams.append((p,ws,ng))\n",
        "      #now we keep only the top k beams (for memory efficiency)\n",
        "      beams = sorted(beams, reverse=True)[:k_beams]\n",
        "    return [ws.replace('_',' ') for _,ws,_ in beams]\n",
        "       \n",
        "  def _tokenise(self,text):\n",
        "    return ''.join([c if ord('a') <= ord(c) <= ord('z') else f' {c} ' for c in text.lower()]).split()\n",
        "  \n",
        "  def _ngrams(self,text):\n",
        "      tokens = self._tokenise(text)\n",
        "      if self.chars: #char-level ngrams\n",
        "        tokens = list(' '.join(tokens).replace(' ','_'))\n",
        "      ngrams = zip(*[tokens[i:] for i in range(self.ngram_size)])\n",
        "      return [\" \".join(ngram) for ngram in ngrams]\n",
        "  \n",
        "  def _normalise(self, X):\n",
        "    s = sum(X)\n",
        "    return [x/s for x in X]\n",
        "  \n",
        "  def _compare(self, string1,string2):\n",
        "    return SequenceMatcher(None,string1, string2).ratio()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM27B3yW_q1c",
        "colab_type": "code",
        "outputId": "5d241fc0-8d1a-4243-dc71-ce5f7cf5b84a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "ng = NgramModel(4)\n",
        "ng.fit(text)\n",
        "ng.generate(\"i like to eat Chinese food\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"i like to eat Chinese food out a new kind of rule , ' and that ' s the most important piece of evidence we ' ve no time to wash the things between whiles .\",\n",
              " \"i like to eat Chinese food these cakes , ' she said to herself , ' i ' m not a serpent , i tell you ! ' said the king . ' when we were\",\n",
              " \"i like to eat Chinese food these cakes , ' she thought , ' till tomorrow - - ' ' i ' m not a serpent , i tell you ! ' said the king .\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipf-3lS7twzp",
        "colab_type": "code",
        "outputId": "45c18c86-b558-4f18-e5f6-8a8a7e357020",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "ng = NgramModel(4, chars=True)\n",
        "ng.fit(text)\n",
        "ng.generate(\"i like to eat Chinese food\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"i like to eat Chinese food down a little , ' said the wa\",\n",
              " \"i like to eat Chinese fooderself , ' said the was she wa\",\n",
              " \"i like to eat Chinese fooderseshought alice , ' said the\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FStxf0fri0Z",
        "colab_type": "text"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOkpsBVZRwyn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "7c2e6c15-2f4a-460a-c489-9e6847204b10"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-21 16:44:49--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-05-21 16:44:49--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2019-05-21 16:44:49--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  11.4MB/s    in 91s     \n",
            "\n",
            "2019-05-21 16:46:21 (9.00 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\n",
            "glove.6B.200d.txt  glove.6B.50d.txt   sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVNg3xs3izpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import keras\n",
        "\n",
        "class LSTMmodel():  \n",
        "  def fit(self,text,epochs=30, wordvec_filepath = 'glove.6B.100d.txt', wordvec_dim = 100):\n",
        "    self._load(text)\n",
        "    self._build(wordvec_filepath, wordvec_dim)\n",
        "    self._train(epochs)\n",
        "\n",
        "  def generate(self,words,i=30):\n",
        "    for _ in range(i):\n",
        "      x = [self.token_idx[token] if token in self.token_idx else 1 for token in self._tokenise(words)] \n",
        "      x = keras.preprocessing.sequence.pad_sequences([x], maxlen=self.x_dim, padding = 'pre')\n",
        "      y_hat = self.model.predict_classes(x, verbose=0)[0] #maximise\n",
        "      words += ' ' + self.idx_token[y_hat]\n",
        "    return words\n",
        "  \n",
        "  def _tokenise(self,text):\n",
        "    return ''.join([c if ord('a') <= ord(c) <= ord('z') else f' {c} ' for c in text.lower()]).split()\n",
        "    \n",
        "  def _load(self, text):    \n",
        "    self.idx_token = dict(enumerate(set(self._tokenise(text)),start=2))\n",
        "    self.idx_token[0] = '<PAD>'\n",
        "    self.idx_token[1] = '<UNK>' \n",
        "    self.token_idx = {word:i for i,word in self.idx_token.items()}       \n",
        "    token_ids = [[self.token_idx[token] for token in self._tokenise(sentence)] for sentence in text.split('\\n')]\n",
        "    inouts = [tokens[:i+1] for tokens in token_ids for i in range(1,len(tokens))]\n",
        "    self.x_dim = max([len(x) for x in inouts]) - 1\n",
        "    self.y_dim = len(self.idx_token) \n",
        "    inouts = np.array(keras.preprocessing.sequence.pad_sequences(inouts,maxlen=self.x_dim + 1, padding='pre'))\n",
        "    self.X, self.Y = inouts[:,:-1], inouts[:,-1]\n",
        "    \n",
        "  def _build(self,filepath,wv_dim):\n",
        "    embeddings = self._embeddingmatrix(self._wordvectors(filepath), wv_dim)\n",
        "    self.model = keras.models.Sequential()\n",
        "    self.model.add(keras.layers.Embedding(self.y_dim, wv_dim, input_length=self.x_dim, embeddings_initializer=keras.initializers.Constant(embeddings) , trainable=False))\n",
        "    self.model.add(keras.layers.LSTM(200, return_sequences = True))\n",
        "    self.model.add(keras.layers.LSTM(200))\n",
        "    self.model.add(keras.layers.Dense(self.y_dim, activation='softmax'))\n",
        "    self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "  def _train(self,epochs):\n",
        "    earlystop =  keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=0, mode='auto')\n",
        "    onehot_y = keras.utils.to_categorical(self.Y, num_classes=self.y_dim)\n",
        "    self.model.fit(self.X, onehot_y, epochs=epochs, verbose=1, callbacks=[earlystop])  \n",
        "  \n",
        "  def _wordvectors(self,file_path):\n",
        "    vectors = {}\n",
        "    with open(file_path) as f:\n",
        "      for line in f.readlines():\n",
        "        values = line.split()\n",
        "        vectors[values[0]] = np.array(values[1:],dtype='float32')\n",
        "    return vectors\n",
        "  \n",
        "  def _embeddingmatrix(self, wordvectors, wv_dim):\n",
        "    matrix = np.zeros((self.y_dim, wv_dim))\n",
        "    for word,idx in self.token_idx.items():\n",
        "      if word.lower() in wordvectors:\n",
        "        matrix[idx] = wordvectors[word.lower()]\n",
        "    return matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqaJLiJHsE3g",
        "colab_type": "code",
        "outputId": "ca897b7e-e395-40a6-91c6-c70a68b8e021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1091
        }
      },
      "source": [
        "lstm = LSTMmodel()\n",
        "lstm.fit(text[1000:10000])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1991/1991 [==============================] - 28s 14ms/step - loss: 5.6688 - acc: 0.0638\n",
            "Epoch 2/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1991/1991 [==============================] - 27s 14ms/step - loss: 5.3495 - acc: 0.0713\n",
            "Epoch 3/30\n",
            "1991/1991 [==============================] - 27s 13ms/step - loss: 5.2858 - acc: 0.0723\n",
            "Epoch 4/30\n",
            "1991/1991 [==============================] - 26s 13ms/step - loss: 5.1999 - acc: 0.0784\n",
            "Epoch 5/30\n",
            "1991/1991 [==============================] - 28s 14ms/step - loss: 5.0569 - acc: 0.0884\n",
            "Epoch 6/30\n",
            "1991/1991 [==============================] - 26s 13ms/step - loss: 4.8775 - acc: 0.1150\n",
            "Epoch 7/30\n",
            "1991/1991 [==============================] - 26s 13ms/step - loss: 4.6495 - acc: 0.1281\n",
            "Epoch 8/30\n",
            "1991/1991 [==============================] - 27s 14ms/step - loss: 4.4180 - acc: 0.1497\n",
            "Epoch 9/30\n",
            "1991/1991 [==============================] - 26s 13ms/step - loss: 4.1730 - acc: 0.1577\n",
            "Epoch 10/30\n",
            "1991/1991 [==============================] - 26s 13ms/step - loss: 3.9347 - acc: 0.1768\n",
            "Epoch 11/30\n",
            "1991/1991 [==============================] - 27s 13ms/step - loss: 3.7110 - acc: 0.2004\n",
            "Epoch 12/30\n",
            "1991/1991 [==============================] - 27s 13ms/step - loss: 3.4891 - acc: 0.2290\n",
            "Epoch 13/30\n",
            "1991/1991 [==============================] - 26s 13ms/step - loss: 3.2763 - acc: 0.2572\n",
            "Epoch 14/30\n",
            "1991/1991 [==============================] - 27s 14ms/step - loss: 3.0543 - acc: 0.2938\n",
            "Epoch 15/30\n",
            "1991/1991 [==============================] - 28s 14ms/step - loss: 2.8529 - acc: 0.3295\n",
            "Epoch 16/30\n",
            "1991/1991 [==============================] - 28s 14ms/step - loss: 2.6552 - acc: 0.3656\n",
            "Epoch 17/30\n",
            "1991/1991 [==============================] - 27s 13ms/step - loss: 2.4479 - acc: 0.4139\n",
            "Epoch 18/30\n",
            "1991/1991 [==============================] - 26s 13ms/step - loss: 2.2570 - acc: 0.4787\n",
            "Epoch 19/30\n",
            "1991/1991 [==============================] - 26s 13ms/step - loss: 2.0683 - acc: 0.5173\n",
            "Epoch 20/30\n",
            "1991/1991 [==============================] - 27s 14ms/step - loss: 1.8839 - acc: 0.5987\n",
            "Epoch 21/30\n",
            "1991/1991 [==============================] - 27s 13ms/step - loss: 1.7024 - acc: 0.6464\n",
            "Epoch 22/30\n",
            "1991/1991 [==============================] - 26s 13ms/step - loss: 1.5323 - acc: 0.7042\n",
            "Epoch 23/30\n",
            "1991/1991 [==============================] - 26s 13ms/step - loss: 1.3839 - acc: 0.7443\n",
            "Epoch 24/30\n",
            "1991/1991 [==============================] - 27s 13ms/step - loss: 1.2299 - acc: 0.7961\n",
            "Epoch 25/30\n",
            "1991/1991 [==============================] - 26s 13ms/step - loss: 1.0942 - acc: 0.8302\n",
            "Epoch 26/30\n",
            "1991/1991 [==============================] - 26s 13ms/step - loss: 0.9724 - acc: 0.8674\n",
            "Epoch 27/30\n",
            "1991/1991 [==============================] - 27s 13ms/step - loss: 0.8561 - acc: 0.8910\n",
            "Epoch 28/30\n",
            "1991/1991 [==============================] - 27s 14ms/step - loss: 0.7552 - acc: 0.9066\n",
            "Epoch 29/30\n",
            "1991/1991 [==============================] - 26s 13ms/step - loss: 0.6624 - acc: 0.9357\n",
            "Epoch 30/30\n",
            "1991/1991 [==============================] - 27s 13ms/step - loss: 0.5846 - acc: 0.9508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-dsu1Tj_BJi",
        "colab_type": "code",
        "outputId": "b6d9328f-5d59-411b-cc46-ab48e16f1e51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "lstm.generate(\"Alice was\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Alice was not a bit hurt , and she jumped up on to her feet in a moment : she looked up , but it was all dark overhead ; and the'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHRcwZeFIRzo",
        "colab_type": "text"
      },
      "source": [
        "## ULMFIT (AWD-LSTM & Transfer Learning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OiEaAFApkwa",
        "colab_type": "code",
        "outputId": "c9fa4d36-1cdd-41e8-9c74-77d208444d1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "training_data = [sentence.split()[:i+1] for sentence in text.split('\\n') for i in range(1,len(sentence.split()))]\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "    \"text\":[' '.join(sample[:-1]) for sample in training_data],\n",
        "    \"label\":[sample[-1] for sample in training_data],\n",
        "})\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I</td>\n",
              "      <td>'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I '</td>\n",
              "      <td>m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I ' m</td>\n",
              "      <td>sure</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I ' m sure</td>\n",
              "      <td>_I_</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I ' m sure _I_</td>\n",
              "      <td>shan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             text label\n",
              "0               I     '\n",
              "1             I '     m\n",
              "2           I ' m  sure\n",
              "3      I ' m sure   _I_\n",
              "4  I ' m sure _I_  shan"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ0X-CZWq04G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.text import language_model_learner, TextLMDataBunch,untar_data,URLs,AWD_LSTM\n",
        "ulmfit = language_model_learner(TextLMDataBunch.from_df(untar_data(URLs.IMDB_SAMPLE),train_df = df , valid_df = df ), AWD_LSTM, drop_mult=.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWmkqFwfOh01",
        "colab_type": "code",
        "outputId": "08b8cdec-d3fd-4bfb-fd3d-11f9ca1106bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "ulmfit.predict(seed, n_words=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'..how was a large - and - large , - off , - Mouse : a ) being over - used when it was found out . The Use Was Sudden That ( a ) was a French by - hands , used to be used for the Rome Times and French Times . In \" The Great Can See , \" the four - used Through The Door of the Being was a French Just'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Kb_kmSzm4IN",
        "colab_type": "code",
        "outputId": "68774fc6-e4e1-4b3b-e3f3-32df4f22ded5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        }
      },
      "source": [
        "ulmfit.fit_one_cycle(1, 1e-2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>7.214114</td>\n",
              "      <td>6.888145</td>\n",
              "      <td>0.014286</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tJNZoX-rAPN",
        "colab_type": "code",
        "outputId": "fba3a6a7-0ab0-438d-c6bf-f5b9f034d322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "ulmfit.predict(seed, n_words=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'..how was in Paris when \" There was a French - French No - Get Me ! right there to see ! \" , he said , \" when i was in the back of the Sea of , i \\' m You i \\' ve Your Eyes \" . The French and English Times They Are a Of The Times , The Times \\' Talk OF The Times ,'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBC92HQjRL4c",
        "colab_type": "text"
      },
      "source": [
        "# Char-Level LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PfJTHiIRP9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import keras\n",
        "\n",
        "class charLSTMmodel():\n",
        "  \n",
        "  def fit(self,text,epochs=100):\n",
        "    self._load(text)\n",
        "    self._build()\n",
        "    self._train(epochs)\n",
        "\n",
        "  def generate(self,words,i=150):\n",
        "    for _ in range(i):\n",
        "      x = [self.token_idx[token] if token in self.token_idx else 1 for token in self._tokenise(words)] \n",
        "      x = keras.preprocessing.sequence.pad_sequences([x], maxlen=self.x_dim, padding = 'pre')\n",
        "      y_hat = self.model.predict_classes(x, verbose=0)[0] #maximise\n",
        "      words += self.idx_token[y_hat]\n",
        "    return words.replace(\"_\",\" \")\n",
        "  \n",
        "  def _chunk(self,text,chunk_size = 100):\n",
        "    return ''.join([c + '<S>' if not i % chunk_size else c for i,c in enumerate(text,start=1)]).split('<S>')\n",
        "  \n",
        "  def _tokenise(self,text):\n",
        "    return list(' '.join(text.split()).replace(\" \",\"_\"))\n",
        "    \n",
        "  def _load(self, text):\n",
        "    self.idx_token = dict(enumerate(set(self._tokenise(text)),start=2))\n",
        "    self.idx_token[0] = '<PAD>'\n",
        "    self.idx_token[1] = '<UNK>' \n",
        "    self.token_idx = {word:i for i,word in self.idx_token.items()}       \n",
        "    token_ids = [[self.token_idx[token] for token in self._tokenise(sentence)] for sentence in self._chunk(text)]\n",
        "    inouts = [tokens[:i+1] for tokens in token_ids for i in range(1,len(tokens))]\n",
        "    self.x_dim = max([len(x) for x in inouts]) - 1\n",
        "    self.y_dim = len(self.idx_token) \n",
        "    inouts = np.array(keras.preprocessing.sequence.pad_sequences(inouts,maxlen=self.x_dim + 1, padding='pre'))\n",
        "    self.X, self.Y = inouts[:,:-1], inouts[:,-1]\n",
        "    \n",
        "  def _build(self):\n",
        "    self.model = keras.models.Sequential()\n",
        "    self.model.add(keras.layers.Embedding(self.y_dim, 10, input_length=self.x_dim))\n",
        "    self.model.add(keras.layers.LSTM(150, return_sequences = True))\n",
        "    self.model.add(keras.layers.LSTM(100))\n",
        "    self.model.add(keras.layers.Dense(self.y_dim, activation='softmax'))\n",
        "    self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "  def _train(self,epochs):\n",
        "    earlystop =  keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "    onehot_y = keras.utils.to_categorical(self.Y, num_classes=self.y_dim)\n",
        "    self.model.fit(self.X, onehot_y, epochs=epochs, verbose=1, callbacks=[earlystop])  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbr1RKGJTAPt",
        "colab_type": "code",
        "outputId": "05cf7faa-bcf9-412e-e6ea-e46ceb965e88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1799
        }
      },
      "source": [
        "clstm = charLSTMmodel()\n",
        "clstm.fit(text, epochs=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "9379/9379 [==============================] - 107s 11ms/step - loss: 3.0769 - acc: 0.2306\n",
            "Epoch 2/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "9379/9379 [==============================] - 105s 11ms/step - loss: 2.9091 - acc: 0.2452\n",
            "Epoch 3/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 2.6316 - acc: 0.2919\n",
            "Epoch 4/50\n",
            "9379/9379 [==============================] - 104s 11ms/step - loss: 2.4382 - acc: 0.3276\n",
            "Epoch 5/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 2.7868 - acc: 0.2730\n",
            "Epoch 6/50\n",
            "9379/9379 [==============================] - 104s 11ms/step - loss: 2.4019 - acc: 0.3371\n",
            "Epoch 7/50\n",
            "9379/9379 [==============================] - 104s 11ms/step - loss: 2.2483 - acc: 0.3688\n",
            "Epoch 8/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 2.1832 - acc: 0.3847\n",
            "Epoch 9/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 2.1257 - acc: 0.3983\n",
            "Epoch 10/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 2.0774 - acc: 0.4100\n",
            "Epoch 11/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 2.0267 - acc: 0.4235\n",
            "Epoch 12/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.9764 - acc: 0.4415\n",
            "Epoch 13/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.9335 - acc: 0.4540\n",
            "Epoch 14/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.8889 - acc: 0.4631\n",
            "Epoch 15/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.8349 - acc: 0.4762\n",
            "Epoch 16/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.8026 - acc: 0.4846\n",
            "Epoch 17/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.7388 - acc: 0.5009\n",
            "Epoch 18/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.6849 - acc: 0.5126\n",
            "Epoch 19/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.6236 - acc: 0.5332\n",
            "Epoch 20/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.5727 - acc: 0.5450\n",
            "Epoch 21/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.5135 - acc: 0.5654\n",
            "Epoch 22/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.4561 - acc: 0.5859\n",
            "Epoch 23/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.3944 - acc: 0.5986\n",
            "Epoch 24/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 1.3369 - acc: 0.6189\n",
            "Epoch 25/50\n",
            "9379/9379 [==============================] - 108s 11ms/step - loss: 1.2779 - acc: 0.6368\n",
            "Epoch 26/50\n",
            "9379/9379 [==============================] - 107s 11ms/step - loss: 1.2222 - acc: 0.6531\n",
            "Epoch 27/50\n",
            "9379/9379 [==============================] - 107s 11ms/step - loss: 1.1953 - acc: 0.6604\n",
            "Epoch 28/50\n",
            "9379/9379 [==============================] - 107s 11ms/step - loss: 1.1136 - acc: 0.6886\n",
            "Epoch 29/50\n",
            "9379/9379 [==============================] - 106s 11ms/step - loss: 1.0562 - acc: 0.7046\n",
            "Epoch 30/50\n",
            "9379/9379 [==============================] - 106s 11ms/step - loss: 0.9876 - acc: 0.7310\n",
            "Epoch 31/50\n",
            "9379/9379 [==============================] - 107s 11ms/step - loss: 0.9406 - acc: 0.7455\n",
            "Epoch 32/50\n",
            "9379/9379 [==============================] - 106s 11ms/step - loss: 0.8806 - acc: 0.7639\n",
            "Epoch 33/50\n",
            "9379/9379 [==============================] - 106s 11ms/step - loss: 0.8281 - acc: 0.7831\n",
            "Epoch 34/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.7728 - acc: 0.7978\n",
            "Epoch 35/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.7213 - acc: 0.8159\n",
            "Epoch 36/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.6710 - acc: 0.8337\n",
            "Epoch 37/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.6233 - acc: 0.8487\n",
            "Epoch 38/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.5783 - acc: 0.8612\n",
            "Epoch 39/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.5450 - acc: 0.8701\n",
            "Epoch 40/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.4916 - acc: 0.8884\n",
            "Epoch 41/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.4432 - acc: 0.9049\n",
            "Epoch 42/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.4108 - acc: 0.9113\n",
            "Epoch 43/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.3782 - acc: 0.9212\n",
            "Epoch 44/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.3434 - acc: 0.9328\n",
            "Epoch 45/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.3201 - acc: 0.9376\n",
            "Epoch 46/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.2767 - acc: 0.9508\n",
            "Epoch 47/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.2487 - acc: 0.9578\n",
            "Epoch 48/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.2315 - acc: 0.9615\n",
            "Epoch 49/50\n",
            "9379/9379 [==============================] - 105s 11ms/step - loss: 0.2193 - acc: 0.9635\n",
            "Epoch 50/50\n",
            "9379/9379 [==============================] - 104s 11ms/step - loss: 0.1955 - acc: 0.9698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5SFStB8MZf7",
        "colab_type": "code",
        "outputId": "f808883a-cdd2-44d9-aa97-ba1c8954992e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "clstm.generate(\"indubitably \")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "indubitably elice , and her eyes filled with tears again . ' You ought to be ashamed of yourself ,' said Alice , ' to speak to this mouse , she was tomes on one h\n",
            "indubitably elice , and her eyes firled with tee whonged I wuber , spe bnowg a warden of yoor again . ' You ought to mriwpy and on the gool of any one , so gone ,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx7w7szoRTCn",
        "colab_type": "text"
      },
      "source": [
        "# char-LSTM with Attention & Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Pfybf0cnF-",
        "colab_type": "code",
        "outputId": "9ec77d6e-9020-4859-db51-f3b8b32954a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from textgenrnn import textgenrnn\n",
        "\n",
        "clstm_att = textgenrnn()\n",
        "clstm_att.generate()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[PS4] LF4M BOOSALIQ a promote of my computer at the mouse of the users made it to the stranger and then the shopping and all the best for the teammates. Thanks for a set of some possible shots at all time his boys on the entire popular decade and she didn't see an increase it would be transferent,\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-8pLZaUjEOq",
        "colab_type": "code",
        "outputId": "839abba4-fb0c-4025-a14b-db4250518d71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5115
        }
      },
      "source": [
        "clstm_att.train_on_texts(text.split('\\n'),num_epochs=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on 9,522 character sequences.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Epoch 1/10\n",
            "74/74 [==============================] - 29s 387ms/step - loss: 1.7788\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "' I had so the she she she she she was not to one of the she she she she she she she she she she was sure the here !'\n",
            "\n",
            "' S sure the she she she she she was surcement to she was sure to she she she she she she she was now in the sure the sure to when the she she she she she she she she was surportility to the she she she she she she she she she she she she was not the sure in the sure in the sure the sure it '  she\n",
            "\n",
            "' Alice '  ' she she she she she she she she she was not such a sure the she she she she she she she was now to the she she she she she she she she she she was not such a little she was now to such she was not such a little surcemen than the how her she was heards , and she was surced to such she \n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "' Rooo and said of the dicress to sure that she was the sure in door !'\n",
            "\n",
            "How to be able the vagin .\n",
            "\n",
            "' And she was such a be now to like and not the said of I want to the she he was on she was doenner , what she she she was have doen the good ; her way was surrives ' seng the begh and she herried , and she was beganded to the same to the planning if a be poor she had a come , '  !'\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Am some doou subiroment hall won don rally , of sunchaathed , engather again .'\n",
            "\n",
            "Howrine ; , '\n",
            "\n",
            "Alice use embart in her please ‪‎sait , lawing sap hemald I house alten like to the great her queer kid rae into the house come hereveever , wake .'\n",
            "\n",
            "Epoch 2/10\n",
            "74/74 [==============================] - 23s 313ms/step - loss: 1.3146\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "' I 'm that she was some to her and she was been the she was a good garden , and she was not some , and she was the same of the same to her way on the sea , and she was a sea and she was to the she seemed to she she was such a she was to the garden is a she she was not such a sure and she was some\n",
            "\n",
            "' We can shall have her and she was her and she was not such a she was to the same that she was some the she was some all the same that the garden and she was some the garden , and she was some to the garden and she was saying to the same that she was such a she was not some all the same and she w\n",
            "\n",
            "' Alice is the sure and she she she was such a sure I was not a sure the same that she was saying the garden , and she was the sea and seemed to her any of the same that she was some the garden , and she was such a she was a good garden , and she was her to the she she was not some , and she was s\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "How Alice , and she was such a half singe in a she she was not some her on to half out the cat about really and she was said to her any the care , and she was had a back that not and she was said a distirnion , I was the little pool of she she she was not seen to see if you had her way it was such\n",
            "\n",
            "' Alice and she she swam her any eyes and she she went on the , and the garden , and she went on little fan , and she was dear , and she was she was said to the sour shand on her same that she went on see her it was all out the mooded for the hall alone was she was her some herself .\n",
            "\n",
            "Howe the way of her was up again , and she was began , and worse in the little seating is , and a golden , and she was every tone and she said to the first in the same that she swill the haur the garden I ' m say her little golden as the side of the carrors , and she was now , and hopel is some so\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "' way in her wade : '  ' l strupse by there , ah to build this bebound , and hama by to the very guite , but I have to tiegle favor in ye side , Williad ' geas and the Nate half only skill her is trying .\n",
            "\n",
            "she the puzzle heard hauchel better again : and hofe hever was I gloves on change , but what and !\n",
            "\n",
            "' Steaaly ' shall begg her she was cause be strict to had it a tuck you went on if she '  thing -- Than : '  O shall her becoverpot at her awessen away Fand a she senden to her if I was sure had we doed in stengle with plate , and funging withing , ' ( Long handrower fat : and I was hassed THI the\n",
            "\n",
            "Epoch 3/10\n",
            "74/74 [==============================] - 23s 313ms/step - loss: 1.1211\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "' Alice thing , and she she was now , and she was now , and she was now , and she she was now and she could say the first to the same to her way of the same to her any of the same and she was now , and she was now , and her she was not the same to she was to  her cat as her any of the same , and h\n",
            "\n",
            "' The Mouse , and she she was now , and she was now , and she she was now , and she was now , and she was the same and she was now , and she she was now to her dear , and she she said her had so her can she was now to her said the care , and she she she was now , and she was to one of the hurrier \n",
            "\n",
            "' We she she said a such a sure any of the same to her hall anytold she she said her said the subject , and she was such a sure and she was now , and she was now , and she was now , and she was now , and she she see she was now , and she she said , and she she said the same that think her she was \n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "clear \" when she soon had one of the , and she went on it , and she had sometome , as the sama , she was paying to any others , and she went on the way of the sea , ' t she said his sounder not the mouse , and her to the samay down herself , and she was such a low and she was to  her planning and \n",
            "\n",
            "' ' s a same and she could was such a long to think and she '  she '  she said Alice , and a sure about to the garden her said ' t said the darkness , and she she was now , she got to one of the book and she could say her any hard to a good really to the same to said her cats , and she she was suc\n",
            "\n",
            "' Mon she had to  and she was to  but that she was a longly as she was now for such a pair of her could say her cat and she she was a long of the darkness , and began , and she she was now , but the cat care , and she was to  I must be a good she was she could don ' s to one of it , and horse she \n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "( a mouse , LGLG Mouse , angly crest mesienhical cried the remarmana to tell of she had to  near a Taber , ' dar , ' let up go to ESO through aimass any she heard out than any way , she such a un again , ' hige offamily , her fier up this she want to her fact tears to pool as me to wood , and waot\n",
            "\n",
            "A be good perlioh againstly ,' a speed planning hauches !)\n",
            "\n",
            "she link as the cy about , said you ' long to not .\n",
            "\n",
            "Epoch 4/10\n",
            "74/74 [==============================] - 23s 315ms/step - loss: 0.9926\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "' We was her way of the Mouse , and she shall have to go to her hall .\n",
            "\n",
            "' We won ' t golden in the garden , and she said , and she was now , and she she soon , ' she said the Mouse , and the poor poor she she shad the fire , and she shall have had now , and she was not the poor sea , and she she she seems to go , and she had herself all the same again , and she was no\n",
            "\n",
            "' We won ' t shall had be a great hall ;-- and she shall have to go ; and she was not a great hall hastily , and she shall have to go to the same her of the same again , and she had be a great hall ;-- -- , and she was now , and she said the Mouse , and she had to  the same age and she was now , a\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "Adon Come out of the hurries , and she feet things , and neats , and she went on the same again , how do I '  the garden down shan nothing , and I '  more of the right way of the Rabbit and she she something to go  the garden , and she was so many to say she ssmort me found only about in the Mouse\n",
            "\n",
            "' I ' ll she she had shaven on the same that she can ' t her said the ' Is a great mut began to her all , and of the Mouse , now , and the Musical , and have has all of the cat again , and hastily , and she was only their hall the same have hoot on the poor indeed , I ' m beg saying speaking ,' sh\n",
            "\n",
            "' We she shall have beganding that she got the day of the cat looked , and side , and she '  must be a dear , and things , and have the little cats .\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "clear , who !\n",
            "\n",
            "twin I pass permanal retrient when has while she she said Alice , began how before , ' for must her mipsed heads , and mots togle , how lix ' voice hear in the hall .\n",
            "\n",
            "what would way asks : burst her fans : ' The ohh herself ; : all , and hurried you 'l a water padball saying had herself , during herself , a Masting ,' that speaking ?\n",
            "\n",
            "Epoch 5/10\n",
            "74/74 [==============================] - 23s 316ms/step - loss: 0.8890\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "' We she was speed to must be a great she had be a good her way offenden !\n",
            "\n",
            "I have to go on the same that she was to one of the same of things , and she had have been changed for the little for the little dear !\n",
            "\n",
            "' We won ' t she see something to the little sea things , and she was up thing that she was not the great her way out of the same and she was nothing that she was now , and she was not the garden !\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "cried the Mouse , and Alice , and the Rabbit seemed to speak that she seems to go that she could have been sand of the hall .\n",
            "\n",
            "I was trying to know the right thing !\n",
            "\n",
            "Try the Mouse is the sudden golden key !\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "I went on the Mons , and looking dooring rapidly am terry and she cass being about .\n",
            "\n",
            "But then you ' d oh !\n",
            "\n",
            "' How doth the starties White thing that would it shall should it !\n",
            "\n",
            "Epoch 6/10\n",
            "74/74 [==============================] - 23s 314ms/step - loss: 0.7942\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "I ' m sure I ' ll stay poor all the poor sech to the same and she was now , and she had now , ' s seemed to himself as he she said , and she said the times she said the times and she was now , and she said , and she said the subject , and she said the same age and she had be a good hall was all ov\n",
            "\n",
            "I ' m sure I ' m sure I ' ll to the same how she was now , and she was now , and she said the times and she said the same age and she she she see the same and she was now , and she was now , and she said , and she said , and she she said , and she said the subject , and she said , and she said , a\n",
            "\n",
            "' I ' m sure I ' m sure I ' m sure .\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "I smok the Mouse , and she shall my notice , do it : and she was to  and she had do : ' the hall was saided the poor dear , and she had to  now , and in a same how began , and she was now , and she was so think about her doesn't speed inches four little safe that she she see so the mouse , how lit\n",
            "\n",
            "Let me think : she looked all the same age into the same , and she she see me feet in the night , that way if I ' m sure ; but the little golden knowledgo are  hffarens , and she same to her fan and seemed to speak .\n",
            "\n",
            "I ' m sure I said the Mouse , and she sways the hall call and she could have been changed for the same , she was now , and she she sees the animation , and have his side , and she could go , and a little so now , and I must be a poor swin her centisu , and she began , and this time she she see s t\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "I could have been conclession !'\n",
            "\n",
            "( Alice are  any clittend the hauch and her phot and litton you ' t ann can come ,' thought if I ' lmm as varda much , s at her poor law words ' O suppose , and in one stow ' S beform ' s dinr kupped .\n",
            "\n",
            "she could go , all spades , and I '  more English ,' clear For , nast , and found out quiting himse .)\n",
            "\n",
            "Epoch 7/10\n",
            "74/74 [==============================] - 24s 318ms/step - loss: 0.7155\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "I ' ll try if I ' m sure I '  , and she was now , and she was now , and she was now , and she was now , and she was now , and she was now , and she was now , and she was now , the little golden key was such a little way , and she was now , and she was now , and she was now , and she went on , and \n",
            "\n",
            "' I won ' t she was to one of the Mouse , and hands , and she was took up a nice of the same age as she went on  With the pool of the same and she was now , and she was now , and she went on going on cried the cat to the glass to the same her way out of the sea , she went on , and she could do  , \n",
            "\n",
            "' We won ' t go and she seems to go to the glass and she went on the same age and she began , and she was now , and she was now , and she was now , and she was now , and she was now , and she went on , and she can see the same , and she was now , and she went on , and she was now , and she was now\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "As she was tight the little golden key in the same , and way off the way of this worse , that I ' l deplay to the poor sending carrier , and she was saying like this took a little way , and a same that were of the pool said Alice , ' , and welcome way .\n",
            "\n",
            "' I '  I shall be a great glad to find her any oh .\n",
            "\n",
            "' How doth the little dear : I think you ' look and she could go , and she was no try and she was now , and is so me and the garden was sure , and she went on , and she could go , ' she soon all she said , that walk how one of its little danch of the garden .\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "' Am I subjuch : Oh must about  Our fan in then , there of a tire familining she went on  growing not , and poor nar understand person .\n",
            "\n",
            "Stop , Ohher I can in a real leah to all she began in this , hope it .\n",
            "\n",
            "' Ahabout bathed , the White Rome cated on concrisions !\n",
            "\n",
            "Epoch 8/10\n",
            "74/74 [==============================] - 24s 329ms/step - loss: 0.6530\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "' We see if I ' m sure I ' ll stay cats , and she was now , and she was now , and she was now , and she went on child , and she was now , and she was now , and she was now , and she was now , and she went on changed again , and she said , and she , ' t she soon and she was now about the first sent\n",
            "\n",
            "I ' m sure I ' ll try in the same , and she was now , and she went on such a narrow and she was now , and she was now , and she went on changed again , and she was digging as he same the poor cat to the same age and she was now about to the same age and she said the little different .\n",
            "\n",
            "I ' m sure I ' m sure _I_ I ' m sure ; I ' ll stay poor animu to the little wgo do it , and she was now , and she was now , and she went on , and she was now , and she was now , and she went on , and she was digging about in the same age and she was now , and she went on the same , and she went on\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "And she went on the same again was the first poor send and see if I ' ll try in a splame , and was to look then the poor cats !\n",
            "\n",
            "' I '  she said these she seems to go by out that it had come trotting and she remembered as he was rather spreainched her , and she went on the time she can dear , and not , she went on , ' O mouse !'\n",
            "\n",
            "she was now !\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "However , at she began , I '  before : but nothing .\n",
            "\n",
            "Send , so mutterial to speak , and she themsions which if yourself is on in existence , ' I have been changed with that wear of all !\n",
            "\n",
            "' Aham Youtubers , I Frese Ice get that ,avey cybed the garden , I ' ll or she could have behavas .'\n",
            "\n",
            "Epoch 9/10\n",
            "74/74 [==============================] - 23s 317ms/step - loss: 0.5864\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "' We do the Mouse -- I ' ll stay down here , and she said the times and she was now , and she was now about to the garden !\n",
            "\n",
            "' We won ' t she see so think you '  she soon she was to go back to the little deal in the pool , and she was now , and she was now , and she was now , and she was now , and she said , and she was such a shrill , and she was now , and she was now , and she said , and she said the try and she began\n",
            "\n",
            "' I won ' t she see something she she was to  the same had happened .\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "' I ' ll seem of as she was to go .\n",
            "\n",
            "And she was so fan to speak , who was triped as he said these , and she will do : \n",
            "\n",
            "I shall be the fire , and she began : in the same when she leap her had not all she had to  out of the time she could have been  , and she was now about it !\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "But then a right is this upseper grint !\n",
            "\n",
            "Perhare not O VEN - We not see become forgot this that way you have !\n",
            "\n",
            "the pool of the Rabbit , and there you had begron hot perht as she won doesn ' t just to  much the care about herself , sheen to speak house .)\n",
            "\n",
            "Epoch 10/10\n",
            "74/74 [==============================] - 24s 319ms/step - loss: 0.5402\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "' I ' m sure I ' ll try in the same age and she was such a sea and water and she was now , and she was now , and she went on the garden , and she was such a sea and was the first sentile , and she said the times again , and she said to say the subject of things was trying to go to the first side o\n",
            "\n",
            "' We do the Mouse was been changed all of his children staying , and I ' m sure I ' ll stay goes to her hands , and she shall have been changed for the first sentence , and she , ' s a sudden change !'\n",
            "\n",
            "' I won ' t be a great great she came : ' the little door was so much as he was now about here , and she seems to go back to the little door : she said the try to the same here all she went on talking !\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "As she went on she knew that I must be worn while she was now , the little golden key of the right , and was wish that side me can the same that she was to go on come ago at all .\n",
            "\n",
            "But will her began with to the little law you '  said Alice , and she hastily , and the fan is thirteing to all over four waiting !'\n",
            "\n",
            "Alice talking to the subject was took up the same of the same , and in the pool !\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Burly she heard near , and the poor of shall have hngp his pridilinci.\n",
            "\n",
            "Pard on I alt how , this nothing had have beganding she wherever !\n",
            "\n",
            "as she wons to fide thing if I 'm m !'\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibZY8T0fiERZ",
        "colab_type": "code",
        "outputId": "df40afc5-e49a-4d12-b3f0-f57268ab0c84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "clstm_att.generate()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Mouse was not as the time she was to one of estist if I ' more , and began to find me , I ' ll male at all the try she stops down the same again .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}