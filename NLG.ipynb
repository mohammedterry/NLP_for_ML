{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLG.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammedterry/NLP_for_ML/blob/master/NLG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "d-o2pBP-4NsX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Gluon NLP"
      ]
    },
    {
      "metadata": {
        "id": "z9tftBbpdspr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "outputId": "8dccb0c7-f32e-49b8-e7a5-85be7c6bdef5"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install mxnet\n",
        "!pip3 install gluonnlp"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mxnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/6d/7932788aea1293015548631495a1b44e588127fedcf4221a5dcdfa411e7b/mxnet-1.4.0.post0-py2.py3-none-manylinux1_x86_64.whl (28.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 28.4MB 912kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.15.0,>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from mxnet) (1.14.6)\n",
            "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Collecting requests>=2.20.0 (from mxnet)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 21.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (2018.11.29)\n",
            "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mgoogle-colab 1.0.0 has requirement requests~=2.18.0, but you'll have requests 2.21.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mfastai 1.0.46 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "Installing collected packages: graphviz, requests, mxnet\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "  Found existing installation: requests 2.18.4\n",
            "    Uninstalling requests-2.18.4:\n",
            "      Successfully uninstalled requests-2.18.4\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.4.0.post0 requests-2.21.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting gluonnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/ab/c2d928aa74fdd029d8ae85988357defbb0e819ea33172e92bd7accdfcc92/gluonnlp-0.5.0.post0.tar.gz (189kB)\n",
            "\u001b[K    100% |████████████████████████████████| 194kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (1.14.6)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/19/c4/f4/70dc14c8b01db12cd4f88c6f71128bb88de545a7690e192ce0\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: gluonnlp\n",
            "Successfully installed gluonnlp-0.5.0.post0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pu3uKGr0dpEJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "c6bc31ff-4d2d-400d-f7c1-1a987f1e27c3"
      },
      "cell_type": "code",
      "source": [
        "import mxnet as mx\n",
        "import gluonnlp as nlp\n",
        "ctx = mx.cpu()\n",
        "lm_model, vocab = nlp.model.get_model(name='awd_lstm_lm_1150', dataset_name='wikitext-2', pretrained=True, ctx=ctx)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab file is not found. Downloading.\n",
            "Downloading /root/.mxnet/models/wikitext-2-be36dc52.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/wikitext-2-be36dc52.zip...\n",
            "Downloading /root/.mxnet/models/awd_lstm_lm_1150_wikitext-2-f9562ed0.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/awd_lstm_lm_1150_wikitext-2-f9562ed0.zip...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4S-pKReDedAZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scorer = nlp.model.BeamSearchScorer(alpha=0, K=5, from_logits=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Atn9dwafef8S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LMDecoder(object):\n",
        "    def __init__(self, model):\n",
        "        self._model = model\n",
        "    def __call__(self, inputs, states):\n",
        "        outputs, states = self._model(mx.nd.expand_dims(inputs, axis=0), states)\n",
        "        return outputs[0], states\n",
        "    def state_info(self, *arg, **kwargs):\n",
        "        return self._model.state_info(*arg, **kwargs)\n",
        "decoder = LMDecoder(lm_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "svjJ3BLCejo2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eos_id = vocab['.']\n",
        "beam_sampler = nlp.model.BeamSearchSampler(beam_size=5,decoder=decoder,eos_id=eos_id,scorer=scorer,max_length=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nP2ATIPLep3x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bos = 'I like the Hulk but Im actually more of a DC fan'.split()\n",
        "bos_ids = [vocab[ele] for ele in bos]\n",
        "begin_states = lm_model.begin_state(batch_size=1, ctx=ctx)\n",
        "if len(bos_ids) > 1:\n",
        "    _, begin_states = lm_model(mx.nd.expand_dims(mx.nd.array(bos_ids[:-1]), axis=1),begin_states)\n",
        "inputs = mx.nd.full(shape=(1,), ctx=ctx, val=bos_ids[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SfUqyRLpevAD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_sequences(sampler, inputs, begin_states, num_print_outcomes):\n",
        "    samples, scores, valid_lengths = sampler(inputs, begin_states)\n",
        "    samples = samples[0].asnumpy()\n",
        "    scores = scores[0].asnumpy()\n",
        "    valid_lengths = valid_lengths[0].asnumpy()\n",
        "    print('Generation Result:')\n",
        "    for i in range(num_print_outcomes):\n",
        "        sentence = bos[:-1]\n",
        "        for ele in samples[i][:valid_lengths[i]]:\n",
        "            sentence.append(vocab.idx_to_token[ele])\n",
        "        print([' '.join(sentence), scores[i]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EH4OBw4pey70",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "5bf61514-4eab-4ae9-9d54-cd133e834de6"
      },
      "cell_type": "code",
      "source": [
        "generate_sequences(beam_sampler, inputs, begin_states, 5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generation Result:\n",
            "['I like the Hulk but Im actually more of a DC fan .', -0.94576716]\n",
            "['I like the Hulk but Im actually more of a DC fan , but it was not until the end of the 20th century that it was not until the end of .', -27.151623]\n",
            "['I like the Hulk but Im actually more of a DC fan , but it was not until the end of the 20th century that it was not the same .', -27.202784]\n",
            "['I like the Hulk but Im actually more of a DC fan , but it was not until the end of the 20th century that it was not until the early 1990s .', -29.516235]\n",
            "['I like the Hulk but Im actually more of a DC fan , but it was not until the end of the 20th century that it was not until the early 1980s .', -29.629818]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X5RrkDLYfNXr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seq_sampler = nlp.model.SequenceSampler(beam_size=5,decoder=decoder,eos_id=eos_id,max_length=100,temperature=0.97)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ha-YLAE9fTcH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "202daeb2-e9db-4a1d-fb09-89ff25a39c23"
      },
      "cell_type": "code",
      "source": [
        "generate_sequences(seq_sampler, inputs, begin_states, 5)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generation Result:\n",
            "['I like the Hulk but Im actually more of a DC fan , throughout Japanese history .', -23.0509]\n",
            "['I like the Hulk but Im actually more of a DC fan like the prime minister , whose mark is \" wild for \" ; casualty and reptile .', -95.719086]\n",
            "['I like the Hulk but Im actually more of a DC fan .', -0.94576716]\n",
            "['I like the Hulk but Im actually more of a DC fan pays off the ground on department <unk> , forcing several or more errors to come drawings , from focusing on <unk> techniques by their own special colors ( and often take down all individual equipment , screenplay and vocals ) .', -213.31241]\n",
            "['I like the Hulk but Im actually more of a DC fan , he also used to desperately submit to the knowledge of the social story of the Māori .', -77.988]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3MV3k0ORgkOz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generate using OpenAI implementation"
      ]
    },
    {
      "metadata": {
        "id": "l8INhHKGgnJb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "outputId": "72c08866-81f1-41c5-9710-6bcca1340c6a"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/openai/gpt-2\n",
        "import os\n",
        "os.chdir('gpt-2')\n",
        "!sh download_model.sh 117M\n",
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 158, done.\u001b[K\n",
            "remote: Total 158 (delta 0), reused 0 (delta 0), pack-reused 158\u001b[K\n",
            "Receiving objects: 100% (158/158), 4.35 MiB | 17.33 MiB/s, done.\n",
            "Resolving deltas: 100% (82/82), done.\n",
            "sh: 0: Can't open download_model.sh\n",
            "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/b7/205702f348aab198baecd1d8344a90748cb68f53bdcd1cc30cbc08e47d3e/fire-0.1.3.tar.gz\n",
            "Collecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K    100% |████████████████████████████████| 604kB 21.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Collecting tqdm==4.31.1 (from -r requirements.txt (line 4))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 20.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.11.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2018.11.29)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.22)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.6)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/2a/1a/4d/6b30377c3051e76559d1185c1dbbfff15aed31f87acdd14c22\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mspacy 2.0.18 has requirement regex==2018.01.10, but you'll have regex 2017.4.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mfastai 1.0.46 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "Installing collected packages: fire, regex, tqdm\n",
            "  Found existing installation: regex 2018.1.10\n",
            "    Uninstalling regex-2018.1.10:\n",
            "      Successfully uninstalled regex-2018.1.10\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed fire-0.1.3 regex-2017.4.5 tqdm-4.31.1\n",
            "Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 86, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 127, in Fire\n",
            "    component_trace = _Fire(component, args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 366, in _Fire\n",
            "    component, remaining_args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 542, in _CallCallable\n",
            "    result = fn(*varargs, **kwargs)\n",
            "  File \"src/interactive_conditional_samples.py\", line 42, in interact_model\n",
            "    enc = encoder.get_encoder(model_name)\n",
            "  File \"/content/gpt-2/src/encoder.py\", line 109, in get_encoder\n",
            "    with open(os.path.join('models', model_name, 'encoder.json'), 'r') as f:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'models/117M/encoder.json'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7NlSbrGqhDbW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "01e6dc6e-6a9a-4bc5-a79d-f33f8934859c"
      },
      "cell_type": "code",
      "source": [
        "!python3 download_model.py 117M"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 615kit/s]                                                      \n",
            "\rFetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\rFetching encoder.json: 1.04Mit [00:00, 46.5Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 616kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:08, 61.7Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 3.50Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 41.1Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 43.1Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CaJUe0ZfhS8f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1312
        },
        "outputId": "93b2a7bf-eea5-4231-f64e-aa50776531ad"
      },
      "cell_type": "code",
      "source": [
        "!python3 src/interactive_conditional_samples.py --nsamples=3 --length=175 --top_k 40 --temperature 0.7"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-03-09 22:14:09.679025: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-03-09 22:14:09.682018: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2829fa0 executing computations on platform Host. Devices:\n",
            "2019-03-09 22:14:09.682079: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Model prompt >>> I like the Hulk but Im actually more of a DC fan \n",
            "======================================== SAMPLE 1 ========================================\n",
            "__________________\n",
            "\n",
            "\"Fifty-nine percent of what you see is what you'll see now.\"\n",
            "\n",
            "\"The greatest thing in the world isn't where we're going to end up but where we're gonna find the next Superman.\"\n",
            "\n",
            "\"I'm not really sure what to make of the news that we're going to end up with Superman. He's been a bit of a disappointment. I think that's why he's been so bad. He's been a failure, I think, because he's so good but he's so bad, because he's so good, because he's so bad, because he's so good.\"\n",
            "\n",
            "\n",
            "What I'm trying to say is, I'm not sure what I'm talking about. I never met a Superman that I thought was good enough for me. I never met a Superman that I thought was good\n",
            "======================================== SAMPLE 2 ========================================\n",
            "ive always been. I am a huge fan of Marvel and I think Marvel is an awesome company. I am gonna be a big fan of DC. They are going to be a really good company. I am a big Marvel fan. DC is a very solid company. I think they are going to be great together. I think I will be a huge Marvel fan.\n",
            "\n",
            "You said you're working on an adaptation of the DC Universe's \"Superman\", and that's your very first time working on an adaptation.\n",
            "\n",
            "Yeah, I love the idea of it. I think the main reason I did it was because I wanted to do a story with a guy who's a superhero. I thought, \"Oh my God, that's what I want to do.\" So a lot of the writing and pacing and characters are going to be a bit different than what\n",
            "======================================== SAMPLE 3 ========================================\n",
            "!!!\n",
            "\n",
            "I am a big fan of his first book and he is also my favorite comic book artist, but I dont know why I dont like him more.\n",
            "\n",
            "I think he is the best character in DC DC Universe. If I could make a list of my biggest DC fans i would be a fan of his first book since he is so much better than the rest.\n",
            "\n",
            "I have a lot of respect for the character of Superman , but I am an agnostic on the character of the comic book.\n",
            "\n",
            "I think he is the best DC character and I like the character of Batman so I am a DC fan. The only thing I can say to be sure is that I loved the Hulk as much as I did his first book.\n",
            "\n",
            "I dont know why I dont like him more.\n",
            "\n",
            "I have a huge love for Superman !\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 5253, in get_controller\n",
            "    yield g\n",
            "  File \"src/interactive_conditional_samples.py\", line 68, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 86, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 127, in Fire\n",
            "    component_trace = _Fire(component, args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 366, in _Fire\n",
            "    component, remaining_args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 542, in _CallCallable\n",
            "    result = fn(*varargs, **kwargs)\n",
            "  File \"src/interactive_conditional_samples.py\", line 83, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1592, in __exit__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 728, in close\n",
            "    tf_session.TF_CloseSession(self._session)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}